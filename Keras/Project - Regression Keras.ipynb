{
    "cells": [
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "<a href=\"https://cognitiveclass.ai/\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/Images/CCLog.png\" width = 200, align = \"center\"></a>\n\n<h1>Build a Regression Model in Keras: Project</h1>"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "## Table of Contents\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n<font size = 3>\n    \n1. <a href=\"#item31\">Download and Clean Dataset</a>  \n2. <a href=\"#item32\">Import Keras</a>  \n3. <a href=\"#item33\">A. Build a baseline model</a>  \n4. <a href=\"#item34\">B. Normalize the data</a>  \n4. <a href=\"#item35\">C. Increate the number of epochs</a> \n4. <a href=\"#item36\">D. Increase the number of hidden layers</a> \n\n</font>\n</div>"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "<a id=\"item31\"></a>"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "## Download and Clean Dataset"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Let's start by importing the <em>pandas</em> and the Numpy libraries."
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "import pandas as pd\nimport numpy as np",
            "execution_count": 1,
            "outputs": []
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "We will be playing around with the same dataset that we used in the videos.\n\n<strong>The dataset is about the compressive strength of different samples of concrete based on the volumes of the different ingredients that were used to make them. Ingredients include:</strong>\n\n<strong>1. Cement</strong>\n\n<strong>2. Blast Furnace Slag</strong>\n\n<strong>3. Fly Ash</strong>\n\n<strong>4. Water</strong>\n\n<strong>5. Superplasticizer</strong>\n\n<strong>6. Coarse Aggregate</strong>\n\n<strong>7. Fine Aggregate</strong>"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Let's download the data and read it into a <em>pandas</em> dataframe."
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "concrete_data = pd.read_csv('https://cocl.us/concrete_data')\nconcrete_data.head()",
            "execution_count": 2,
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n      <th>Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.89</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.27</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.05</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.30</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0   540.0                 0.0      0.0  162.0               2.5   \n1   540.0                 0.0      0.0  162.0               2.5   \n2   332.5               142.5      0.0  228.0               0.0   \n3   332.5               142.5      0.0  228.0               0.0   \n4   198.6               132.4      0.0  192.0               0.0   \n\n   Coarse Aggregate  Fine Aggregate  Age  Strength  \n0            1040.0           676.0   28     79.99  \n1            1055.0           676.0   28     61.89  \n2             932.0           594.0  270     40.27  \n3             932.0           594.0  365     41.05  \n4             978.4           825.5  360     44.30  "
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "So the first concrete sample has 540 cubic meter of cement, 0 cubic meter of blast furnace slag, 0 cubic meter of fly ash, 162 cubic meter of water, 2.5 cubic meter of superplaticizer, 1040 cubic meter of coarse aggregate, 676 cubic meter of fine aggregate. Such a concrete mix which is 28 days old, has a compressive strength of 79.99 MPa. "
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "#### Let's check how many data points we have."
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "concrete_data.shape",
            "execution_count": 3,
            "outputs": [
                {
                    "data": {
                        "text/plain": "(1030, 9)"
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "So, there are approximately 1000 samples to train our model on. Because of the few samples, we have to be careful not to overfit the training data."
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Let's check the dataset for any missing values."
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "concrete_data.describe()",
            "execution_count": 4,
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n      <th>Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>281.167864</td>\n      <td>73.895825</td>\n      <td>54.188350</td>\n      <td>181.567282</td>\n      <td>6.204660</td>\n      <td>972.918932</td>\n      <td>773.580485</td>\n      <td>45.662136</td>\n      <td>35.817961</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>104.506364</td>\n      <td>86.279342</td>\n      <td>63.997004</td>\n      <td>21.354219</td>\n      <td>5.973841</td>\n      <td>77.753954</td>\n      <td>80.175980</td>\n      <td>63.169912</td>\n      <td>16.705742</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>102.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>121.800000</td>\n      <td>0.000000</td>\n      <td>801.000000</td>\n      <td>594.000000</td>\n      <td>1.000000</td>\n      <td>2.330000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>192.375000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>164.900000</td>\n      <td>0.000000</td>\n      <td>932.000000</td>\n      <td>730.950000</td>\n      <td>7.000000</td>\n      <td>23.710000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>272.900000</td>\n      <td>22.000000</td>\n      <td>0.000000</td>\n      <td>185.000000</td>\n      <td>6.400000</td>\n      <td>968.000000</td>\n      <td>779.500000</td>\n      <td>28.000000</td>\n      <td>34.445000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>350.000000</td>\n      <td>142.950000</td>\n      <td>118.300000</td>\n      <td>192.000000</td>\n      <td>10.200000</td>\n      <td>1029.400000</td>\n      <td>824.000000</td>\n      <td>56.000000</td>\n      <td>46.135000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>540.000000</td>\n      <td>359.400000</td>\n      <td>200.100000</td>\n      <td>247.000000</td>\n      <td>32.200000</td>\n      <td>1145.000000</td>\n      <td>992.600000</td>\n      <td>365.000000</td>\n      <td>82.600000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "            Cement  Blast Furnace Slag      Fly Ash        Water  \\\ncount  1030.000000         1030.000000  1030.000000  1030.000000   \nmean    281.167864           73.895825    54.188350   181.567282   \nstd     104.506364           86.279342    63.997004    21.354219   \nmin     102.000000            0.000000     0.000000   121.800000   \n25%     192.375000            0.000000     0.000000   164.900000   \n50%     272.900000           22.000000     0.000000   185.000000   \n75%     350.000000          142.950000   118.300000   192.000000   \nmax     540.000000          359.400000   200.100000   247.000000   \n\n       Superplasticizer  Coarse Aggregate  Fine Aggregate          Age  \\\ncount       1030.000000       1030.000000     1030.000000  1030.000000   \nmean           6.204660        972.918932      773.580485    45.662136   \nstd            5.973841         77.753954       80.175980    63.169912   \nmin            0.000000        801.000000      594.000000     1.000000   \n25%            0.000000        932.000000      730.950000     7.000000   \n50%            6.400000        968.000000      779.500000    28.000000   \n75%           10.200000       1029.400000      824.000000    56.000000   \nmax           32.200000       1145.000000      992.600000   365.000000   \n\n          Strength  \ncount  1030.000000  \nmean     35.817961  \nstd      16.705742  \nmin       2.330000  \n25%      23.710000  \n50%      34.445000  \n75%      46.135000  \nmax      82.600000  "
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "concrete_data.isnull().sum()",
            "execution_count": 5,
            "outputs": [
                {
                    "data": {
                        "text/plain": "Cement                0\nBlast Furnace Slag    0\nFly Ash               0\nWater                 0\nSuperplasticizer      0\nCoarse Aggregate      0\nFine Aggregate        0\nAge                   0\nStrength              0\ndtype: int64"
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "The data looks very clean and is ready to be used to build our model."
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "#### Split data into predictors and target"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The target variable in this problem is the concrete sample strength. Therefore, our predictors will be all the other columns."
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "concrete_data_columns = concrete_data.columns\n\npredictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\ntarget = concrete_data['Strength'] # Strength column",
            "execution_count": 6,
            "outputs": []
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "<a id=\"item2\"></a>"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Let's do a quick sanity check of the predictors and the target dataframes."
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "predictors.head()",
            "execution_count": 7,
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0   540.0                 0.0      0.0  162.0               2.5   \n1   540.0                 0.0      0.0  162.0               2.5   \n2   332.5               142.5      0.0  228.0               0.0   \n3   332.5               142.5      0.0  228.0               0.0   \n4   198.6               132.4      0.0  192.0               0.0   \n\n   Coarse Aggregate  Fine Aggregate  Age  \n0            1040.0           676.0   28  \n1            1055.0           676.0   28  \n2             932.0           594.0  270  \n3             932.0           594.0  365  \n4             978.4           825.5  360  "
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "target.head()",
            "execution_count": 8,
            "outputs": [
                {
                    "data": {
                        "text/plain": "0    79.99\n1    61.89\n2    40.27\n3    41.05\n4    44.30\nName: Strength, dtype: float64"
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "n_cols = predictors.shape[1] # number of predictors\nn_cols",
            "execution_count": 9,
            "outputs": [
                {
                    "data": {
                        "text/plain": "8"
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Finally, the last step is to normalize the data by substracting the mean and dividing by the standard deviation."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"item32\"></a>"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "## Import Keras"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Recall from the videos that Keras normally runs on top of a low-level library such as TensorFlow. This means that to be able to use the Keras library, you will have to install TensorFlow first and when you import the Keras library, it will be explicitly displayed what backend was used to install the Keras library. In CC Labs, we used TensorFlow as the backend to install Keras, so it should clearly print that when we import Keras."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#!pip install keras\n#!pip install tensorflow",
            "execution_count": 10,
            "outputs": []
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "#### Let's go ahead and import the Keras library"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "import keras",
            "execution_count": 11,
            "outputs": []
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "As you can see, the TensorFlow backend was used to install the Keras library."
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Let's import the rest of the packages from the Keras library that we will need to build our regressoin model."
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "from keras.models import Sequential\nfrom keras.layers import Dense",
            "execution_count": 12,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"item33\"></a>"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## A. Build a baseline model"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "### Build a Neural Network"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Let's define a function that defines our regression model for us so that we can conveniently call it to create our model."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": " - One hidden layer of 10 nodes, and a ReLU activation function\n - Use the adam optimizer and the mean squared error as the loss function."
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "# define regression model\ndef regression_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n    model.add(Dense(1))\n    \n    # compile model\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model",
            "execution_count": 13,
            "outputs": []
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "### Train and Test the Network"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Next, we will train and test the model at the same time using the *fit* method. We will leave out 30% of the data for validation and we will train the model for 50 epochs."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "# fit the model\ndf = pd.DataFrame()\nrepeats = 3\nfor repeat in range(repeats):\n    model = regression_model()  # build the model\n    history_callback = model.fit(predictors, target, validation_split=0.3, epochs=50, verbose=2)\n    \n    layer_name = 'repeat_{}'.format(repeat + 1) # otherwise give the layer a number\n    df[layer_name] = history_callback.history[\"loss\"]\n    \n    if repeat == repeats-1 :\n        df['mean'] = df.mean(axis=1)\n        df['std'] = df.iloc[:,0:repeats].std(axis=1)",
            "execution_count": 14,
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/50\n23/23 - 0s - loss: 34429.6797 - val_loss: 7205.2549\nEpoch 2/50\n23/23 - 0s - loss: 5892.3252 - val_loss: 2726.6519\nEpoch 3/50\n23/23 - 0s - loss: 4111.5044 - val_loss: 2064.3208\nEpoch 4/50\n23/23 - 0s - loss: 3240.2229 - val_loss: 1603.2306\nEpoch 5/50\n23/23 - 0s - loss: 2621.3755 - val_loss: 1434.1145\nEpoch 6/50\n23/23 - 0s - loss: 2132.5864 - val_loss: 1284.9808\nEpoch 7/50\n23/23 - 0s - loss: 1746.0953 - val_loss: 1169.6035\nEpoch 8/50\n23/23 - 0s - loss: 1450.3196 - val_loss: 1043.3615\nEpoch 9/50\n23/23 - 0s - loss: 1231.8783 - val_loss: 925.0154\nEpoch 10/50\n23/23 - 0s - loss: 1048.6973 - val_loss: 837.0942\nEpoch 11/50\n23/23 - 0s - loss: 897.5958 - val_loss: 747.5916\nEpoch 12/50\n23/23 - 0s - loss: 779.7368 - val_loss: 681.8558\nEpoch 13/50\n23/23 - 0s - loss: 682.2324 - val_loss: 609.2440\nEpoch 14/50\n23/23 - 0s - loss: 604.9186 - val_loss: 557.6312\nEpoch 15/50\n23/23 - 0s - loss: 539.4342 - val_loss: 507.1354\nEpoch 16/50\n23/23 - 0s - loss: 488.8971 - val_loss: 455.3516\nEpoch 17/50\n23/23 - 0s - loss: 445.8291 - val_loss: 401.9056\nEpoch 18/50\n23/23 - 0s - loss: 409.9479 - val_loss: 377.7630\nEpoch 19/50\n23/23 - 0s - loss: 380.1180 - val_loss: 344.8301\nEpoch 20/50\n23/23 - 0s - loss: 354.2055 - val_loss: 308.9185\nEpoch 21/50\n23/23 - 0s - loss: 329.5909 - val_loss: 283.9872\nEpoch 22/50\n23/23 - 0s - loss: 310.1677 - val_loss: 257.4027\nEpoch 23/50\n23/23 - 0s - loss: 295.1990 - val_loss: 228.8387\nEpoch 24/50\n23/23 - 0s - loss: 275.6502 - val_loss: 217.5518\nEpoch 25/50\n23/23 - 0s - loss: 261.5623 - val_loss: 201.2223\nEpoch 26/50\n23/23 - 0s - loss: 248.7703 - val_loss: 191.2460\nEpoch 27/50\n23/23 - 0s - loss: 240.9747 - val_loss: 183.5337\nEpoch 28/50\n23/23 - 0s - loss: 228.1331 - val_loss: 166.2012\nEpoch 29/50\n23/23 - 0s - loss: 218.9046 - val_loss: 153.2643\nEpoch 30/50\n23/23 - 0s - loss: 210.7977 - val_loss: 149.5598\nEpoch 31/50\n23/23 - 0s - loss: 205.2626 - val_loss: 160.3278\nEpoch 32/50\n23/23 - 0s - loss: 197.6014 - val_loss: 134.4824\nEpoch 33/50\n23/23 - 0s - loss: 191.6396 - val_loss: 132.0682\nEpoch 34/50\n23/23 - 0s - loss: 186.3956 - val_loss: 136.5447\nEpoch 35/50\n23/23 - 0s - loss: 183.4308 - val_loss: 126.4278\nEpoch 36/50\n23/23 - 0s - loss: 178.4207 - val_loss: 132.4728\nEpoch 37/50\n23/23 - 0s - loss: 173.7951 - val_loss: 120.0261\nEpoch 38/50\n23/23 - 0s - loss: 170.1265 - val_loss: 136.2998\nEpoch 39/50\n23/23 - 0s - loss: 168.4558 - val_loss: 108.0520\nEpoch 40/50\n23/23 - 0s - loss: 167.5293 - val_loss: 119.9472\nEpoch 41/50\n23/23 - 0s - loss: 162.9899 - val_loss: 137.2991\nEpoch 42/50\n23/23 - 0s - loss: 160.8317 - val_loss: 122.6760\nEpoch 43/50\n23/23 - 0s - loss: 159.9274 - val_loss: 132.1595\nEpoch 44/50\n23/23 - 0s - loss: 157.4582 - val_loss: 118.0265\nEpoch 45/50\n23/23 - 0s - loss: 156.7685 - val_loss: 132.4281\nEpoch 46/50\n23/23 - 0s - loss: 153.9549 - val_loss: 127.6930\nEpoch 47/50\n23/23 - 0s - loss: 154.3288 - val_loss: 117.2358\nEpoch 48/50\n23/23 - 0s - loss: 152.0489 - val_loss: 116.0240\nEpoch 49/50\n23/23 - 0s - loss: 153.0349 - val_loss: 117.9204\nEpoch 50/50\n23/23 - 0s - loss: 150.0411 - val_loss: 135.7833\nEpoch 1/50\n23/23 - 0s - loss: 87992.4609 - val_loss: 39509.9180\nEpoch 2/50\n23/23 - 0s - loss: 23292.7051 - val_loss: 9503.8916\nEpoch 3/50\n23/23 - 0s - loss: 7904.0317 - val_loss: 7490.3457\nEpoch 4/50\n23/23 - 0s - loss: 6853.4590 - val_loss: 6524.2588\nEpoch 5/50\n23/23 - 0s - loss: 6083.7080 - val_loss: 5846.0776\nEpoch 6/50\n23/23 - 0s - loss: 5448.1738 - val_loss: 5249.4146\nEpoch 7/50\n23/23 - 0s - loss: 4906.3989 - val_loss: 4663.0444\nEpoch 8/50\n23/23 - 0s - loss: 4361.5244 - val_loss: 4153.3384\nEpoch 9/50\n23/23 - 0s - loss: 3906.0261 - val_loss: 3695.5234\nEpoch 10/50\n23/23 - 0s - loss: 3509.0867 - val_loss: 3291.8757\nEpoch 11/50\n23/23 - 0s - loss: 3160.4370 - val_loss: 2932.4812\nEpoch 12/50\n23/23 - 0s - loss: 2816.3694 - val_loss: 2612.3508\nEpoch 13/50\n23/23 - 0s - loss: 2545.9717 - val_loss: 2334.7188\nEpoch 14/50\n23/23 - 0s - loss: 2300.5452 - val_loss: 2086.9363\nEpoch 15/50\n23/23 - 0s - loss: 2083.7590 - val_loss: 1867.8038\nEpoch 16/50\n23/23 - 0s - loss: 1888.8433 - val_loss: 1692.4821\nEpoch 17/50\n23/23 - 0s - loss: 1720.9877 - val_loss: 1515.8446\nEpoch 18/50\n23/23 - 0s - loss: 1565.0884 - val_loss: 1375.1115\nEpoch 19/50\n23/23 - 0s - loss: 1424.7751 - val_loss: 1244.1300\nEpoch 20/50\n23/23 - 0s - loss: 1302.5065 - val_loss: 1125.0713\nEpoch 21/50\n23/23 - 0s - loss: 1189.8165 - val_loss: 1020.6990\nEpoch 22/50\n23/23 - 0s - loss: 1090.5372 - val_loss: 928.3875\nEpoch 23/50\n23/23 - 0s - loss: 1002.8057 - val_loss: 850.6105\nEpoch 24/50\n23/23 - 0s - loss: 915.6082 - val_loss: 776.7559\nEpoch 25/50\n23/23 - 0s - loss: 839.4113 - val_loss: 713.7335\nEpoch 26/50\n23/23 - 0s - loss: 773.9779 - val_loss: 657.4348\nEpoch 27/50\n23/23 - 0s - loss: 711.8581 - val_loss: 604.6031\nEpoch 28/50\n23/23 - 0s - loss: 655.7937 - val_loss: 553.5010\nEpoch 29/50\n23/23 - 0s - loss: 604.2448 - val_loss: 509.4134\nEpoch 30/50\n23/23 - 0s - loss: 558.3162 - val_loss: 470.5692\nEpoch 31/50\n23/23 - 0s - loss: 514.1758 - val_loss: 435.1526\nEpoch 32/50\n23/23 - 0s - loss: 475.8472 - val_loss: 400.9503\nEpoch 33/50\n23/23 - 0s - loss: 440.7728 - val_loss: 371.2993\nEpoch 34/50\n23/23 - 0s - loss: 408.4418 - val_loss: 346.6695\nEpoch 35/50\n23/23 - 0s - loss: 379.9509 - val_loss: 322.6194\nEpoch 36/50\n23/23 - 0s - loss: 352.5593 - val_loss: 299.8708\nEpoch 37/50\n23/23 - 0s - loss: 328.5758 - val_loss: 279.5306\nEpoch 38/50\n23/23 - 0s - loss: 306.2884 - val_loss: 260.6590\nEpoch 39/50\n23/23 - 0s - loss: 286.7425 - val_loss: 245.8410\nEpoch 40/50\n23/23 - 0s - loss: 268.3448 - val_loss: 232.1528\nEpoch 41/50\n23/23 - 0s - loss: 251.9066 - val_loss: 216.5448\nEpoch 42/50\n23/23 - 0s - loss: 238.0287 - val_loss: 205.6450\nEpoch 43/50\n23/23 - 0s - loss: 221.0850 - val_loss: 194.2759\nEpoch 44/50\n23/23 - 0s - loss: 209.0593 - val_loss: 185.3978\nEpoch 45/50\n23/23 - 0s - loss: 197.8880 - val_loss: 178.2567\nEpoch 46/50\n23/23 - 0s - loss: 189.1807 - val_loss: 167.2718\nEpoch 47/50\n23/23 - 0s - loss: 179.4844 - val_loss: 160.7594\nEpoch 48/50\n23/23 - 0s - loss: 172.4882 - val_loss: 152.9289\nEpoch 49/50\n23/23 - 0s - loss: 164.8585 - val_loss: 146.7626\nEpoch 50/50\n23/23 - 0s - loss: 158.6013 - val_loss: 142.9259\nEpoch 1/50\n23/23 - 0s - loss: 78462.7500 - val_loss: 57234.1602\nEpoch 2/50\n23/23 - 0s - loss: 46373.9297 - val_loss: 31605.0391\nEpoch 3/50\n23/23 - 0s - loss: 23883.5762 - val_loss: 13781.0049\nEpoch 4/50\n23/23 - 0s - loss: 9134.2969 - val_loss: 3686.5830\nEpoch 5/50\n23/23 - 0s - loss: 2640.5745 - val_loss: 1194.9072\nEpoch 6/50\n23/23 - 0s - loss: 1680.8925 - val_loss: 1096.9277\nEpoch 7/50\n23/23 - 0s - loss: 1601.3849 - val_loss: 1042.2080\nEpoch 8/50\n23/23 - 0s - loss: 1525.9015 - val_loss: 996.8879\nEpoch 9/50\n23/23 - 0s - loss: 1459.4341 - val_loss: 950.9285\nEpoch 10/50\n23/23 - 0s - loss: 1393.4785 - val_loss: 905.9247\nEpoch 11/50\n23/23 - 0s - loss: 1329.2524 - val_loss: 863.5195\nEpoch 12/50\n23/23 - 0s - loss: 1270.1073 - val_loss: 824.4562\nEpoch 13/50\n23/23 - 0s - loss: 1212.2539 - val_loss: 786.9167\nEpoch 14/50\n23/23 - 0s - loss: 1160.6787 - val_loss: 753.1432\nEpoch 15/50\n23/23 - 0s - loss: 1107.3772 - val_loss: 715.8448\nEpoch 16/50\n23/23 - 0s - loss: 1049.8647 - val_loss: 687.3206\nEpoch 17/50\n23/23 - 0s - loss: 1005.6332 - val_loss: 652.4058\nEpoch 18/50\n23/23 - 0s - loss: 957.2100 - val_loss: 623.3554\nEpoch 19/50\n23/23 - 0s - loss: 909.9783 - val_loss: 594.0143\nEpoch 20/50\n23/23 - 0s - loss: 868.0010 - val_loss: 565.6618\nEpoch 21/50\n23/23 - 0s - loss: 826.7979 - val_loss: 539.3500\nEpoch 22/50\n23/23 - 0s - loss: 785.9138 - val_loss: 516.1538\nEpoch 23/50\n23/23 - 0s - loss: 749.6844 - val_loss: 494.5484\nEpoch 24/50\n23/23 - 0s - loss: 713.3459 - val_loss: 468.1284\nEpoch 25/50\n23/23 - 0s - loss: 681.8749 - val_loss: 450.7142\nEpoch 26/50\n23/23 - 0s - loss: 645.9104 - val_loss: 426.9297\nEpoch 27/50\n23/23 - 0s - loss: 614.8051 - val_loss: 408.0831\nEpoch 28/50\n23/23 - 0s - loss: 589.2800 - val_loss: 390.9389\nEpoch 29/50\n23/23 - 0s - loss: 554.3098 - val_loss: 372.2847\nEpoch 30/50\n23/23 - 0s - loss: 529.2248 - val_loss: 355.2816\nEpoch 31/50\n23/23 - 0s - loss: 502.5332 - val_loss: 342.2168\nEpoch 32/50\n23/23 - 0s - loss: 479.9257 - val_loss: 326.3910\nEpoch 33/50\n23/23 - 0s - loss: 455.0547 - val_loss: 313.8607\nEpoch 34/50\n23/23 - 0s - loss: 433.4394 - val_loss: 301.0661\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 35/50\n23/23 - 0s - loss: 413.6663 - val_loss: 288.8494\nEpoch 36/50\n23/23 - 0s - loss: 394.5992 - val_loss: 279.2250\nEpoch 37/50\n23/23 - 0s - loss: 376.9286 - val_loss: 270.0122\nEpoch 38/50\n23/23 - 0s - loss: 360.7698 - val_loss: 259.3181\nEpoch 39/50\n23/23 - 0s - loss: 344.3469 - val_loss: 251.2223\nEpoch 40/50\n23/23 - 0s - loss: 330.0983 - val_loss: 242.2477\nEpoch 41/50\n23/23 - 0s - loss: 317.0658 - val_loss: 236.1766\nEpoch 42/50\n23/23 - 0s - loss: 305.3062 - val_loss: 227.2607\nEpoch 43/50\n23/23 - 0s - loss: 293.7560 - val_loss: 221.3172\nEpoch 44/50\n23/23 - 0s - loss: 282.2054 - val_loss: 214.3303\nEpoch 45/50\n23/23 - 0s - loss: 272.8712 - val_loss: 210.5025\nEpoch 46/50\n23/23 - 0s - loss: 267.5848 - val_loss: 209.8278\nEpoch 47/50\n23/23 - 0s - loss: 257.6196 - val_loss: 196.7141\nEpoch 48/50\n23/23 - 0s - loss: 246.5809 - val_loss: 194.3892\nEpoch 49/50\n23/23 - 0s - loss: 239.3233 - val_loss: 186.1276\nEpoch 50/50\n23/23 - 0s - loss: 232.6182 - val_loss: 182.0730\n"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Report the mean and the standard deviation of the mean squared errors."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "print(df.shape)\ndf",
            "execution_count": 15,
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "(50, 5)\n"
                },
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>repeat_1</th>\n      <th>repeat_2</th>\n      <th>repeat_3</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>34429.679688</td>\n      <td>87992.460938</td>\n      <td>78462.750000</td>\n      <td>66961.630208</td>\n      <td>28573.583975</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5892.325195</td>\n      <td>23292.705078</td>\n      <td>46373.929688</td>\n      <td>25186.319987</td>\n      <td>20307.127040</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4111.504395</td>\n      <td>7904.031738</td>\n      <td>23883.576172</td>\n      <td>11966.370768</td>\n      <td>10493.362397</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3240.222900</td>\n      <td>6853.458984</td>\n      <td>9134.296875</td>\n      <td>6409.326253</td>\n      <td>2972.030857</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2621.375488</td>\n      <td>6083.708008</td>\n      <td>2640.574463</td>\n      <td>3781.885986</td>\n      <td>1993.459459</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2132.586426</td>\n      <td>5448.173828</td>\n      <td>1680.892456</td>\n      <td>3087.217570</td>\n      <td>2057.083542</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1746.095337</td>\n      <td>4906.398926</td>\n      <td>1601.384888</td>\n      <td>2751.293050</td>\n      <td>1867.778434</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1450.319580</td>\n      <td>4361.524414</td>\n      <td>1525.901489</td>\n      <td>2445.915161</td>\n      <td>1659.396657</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1231.878296</td>\n      <td>3906.026123</td>\n      <td>1459.434082</td>\n      <td>2199.112834</td>\n      <td>1482.602490</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1048.697266</td>\n      <td>3509.086670</td>\n      <td>1393.478516</td>\n      <td>1983.754150</td>\n      <td>1332.177916</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>897.595764</td>\n      <td>3160.437012</td>\n      <td>1329.252441</td>\n      <td>1795.761739</td>\n      <td>1201.389122</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>779.736755</td>\n      <td>2816.369385</td>\n      <td>1270.107300</td>\n      <td>1622.071147</td>\n      <td>1062.956739</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>682.232422</td>\n      <td>2545.971680</td>\n      <td>1212.253906</td>\n      <td>1480.152669</td>\n      <td>960.316779</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>604.918579</td>\n      <td>2300.545166</td>\n      <td>1160.678711</td>\n      <td>1355.380819</td>\n      <td>864.418338</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>539.434204</td>\n      <td>2083.759033</td>\n      <td>1107.377197</td>\n      <td>1243.523478</td>\n      <td>781.112445</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>488.897095</td>\n      <td>1888.843262</td>\n      <td>1049.864746</td>\n      <td>1142.535034</td>\n      <td>704.558837</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>445.829132</td>\n      <td>1720.987671</td>\n      <td>1005.633240</td>\n      <td>1057.483348</td>\n      <td>639.158548</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>409.947906</td>\n      <td>1565.088379</td>\n      <td>957.209961</td>\n      <td>977.415415</td>\n      <td>577.835247</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>380.117981</td>\n      <td>1424.775146</td>\n      <td>909.978333</td>\n      <td>904.957153</td>\n      <td>522.346683</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>354.205505</td>\n      <td>1302.506470</td>\n      <td>868.000977</td>\n      <td>841.570984</td>\n      <td>474.702631</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>329.590881</td>\n      <td>1189.816528</td>\n      <td>826.797852</td>\n      <td>782.068420</td>\n      <td>431.853659</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>310.167694</td>\n      <td>1090.537231</td>\n      <td>785.913757</td>\n      <td>728.872894</td>\n      <td>393.299375</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>295.198975</td>\n      <td>1002.805664</td>\n      <td>749.684448</td>\n      <td>682.563029</td>\n      <td>358.546748</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>275.650177</td>\n      <td>915.608154</td>\n      <td>713.345947</td>\n      <td>634.868093</td>\n      <td>327.117155</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>261.562286</td>\n      <td>839.411255</td>\n      <td>681.874939</td>\n      <td>594.282827</td>\n      <td>298.716657</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>248.770264</td>\n      <td>773.977905</td>\n      <td>645.910400</td>\n      <td>556.219523</td>\n      <td>273.850519</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>240.974731</td>\n      <td>711.858093</td>\n      <td>614.805115</td>\n      <td>522.545980</td>\n      <td>248.629435</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>228.133133</td>\n      <td>655.793701</td>\n      <td>589.280029</td>\n      <td>491.068954</td>\n      <td>230.124861</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>218.904587</td>\n      <td>604.244812</td>\n      <td>554.309753</td>\n      <td>459.153051</td>\n      <td>209.553981</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>210.797653</td>\n      <td>558.316162</td>\n      <td>529.224792</td>\n      <td>432.779536</td>\n      <td>192.791452</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>205.262573</td>\n      <td>514.175781</td>\n      <td>502.533203</td>\n      <td>407.323853</td>\n      <td>175.087001</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>197.601379</td>\n      <td>475.847168</td>\n      <td>479.925659</td>\n      <td>384.458069</td>\n      <td>161.835488</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>191.639633</td>\n      <td>440.772766</td>\n      <td>455.054749</td>\n      <td>362.489049</td>\n      <td>148.132157</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>186.395630</td>\n      <td>408.441833</td>\n      <td>433.439392</td>\n      <td>342.758952</td>\n      <td>135.990205</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>183.430801</td>\n      <td>379.950897</td>\n      <td>413.666290</td>\n      <td>325.682663</td>\n      <td>124.341771</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>178.420654</td>\n      <td>352.559296</td>\n      <td>394.599213</td>\n      <td>308.526388</td>\n      <td>114.618781</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>173.795105</td>\n      <td>328.575836</td>\n      <td>376.928619</td>\n      <td>293.099854</td>\n      <td>106.111805</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>170.126541</td>\n      <td>306.288361</td>\n      <td>360.769836</td>\n      <td>279.061579</td>\n      <td>98.194652</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>168.455841</td>\n      <td>286.742523</td>\n      <td>344.346924</td>\n      <td>266.515096</td>\n      <td>89.673184</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>167.529251</td>\n      <td>268.344849</td>\n      <td>330.098328</td>\n      <td>255.324142</td>\n      <td>82.062965</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>162.989899</td>\n      <td>251.906570</td>\n      <td>317.065826</td>\n      <td>243.987432</td>\n      <td>77.342630</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>160.831726</td>\n      <td>238.028671</td>\n      <td>305.306244</td>\n      <td>234.722214</td>\n      <td>72.293991</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>159.927429</td>\n      <td>221.084991</td>\n      <td>293.756012</td>\n      <td>224.922811</td>\n      <td>66.996784</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>157.458221</td>\n      <td>209.059341</td>\n      <td>282.205414</td>\n      <td>216.240992</td>\n      <td>62.682913</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>156.768509</td>\n      <td>197.887955</td>\n      <td>272.871155</td>\n      <td>209.175873</td>\n      <td>58.868658</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>153.954926</td>\n      <td>189.180740</td>\n      <td>267.584778</td>\n      <td>203.573481</td>\n      <td>58.166133</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>154.328766</td>\n      <td>179.484406</td>\n      <td>257.619568</td>\n      <td>197.144246</td>\n      <td>53.862324</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>152.048874</td>\n      <td>172.488205</td>\n      <td>246.580948</td>\n      <td>190.372676</td>\n      <td>49.739009</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>153.034866</td>\n      <td>164.858536</td>\n      <td>239.323318</td>\n      <td>185.738907</td>\n      <td>46.780516</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>150.041107</td>\n      <td>158.601318</td>\n      <td>232.618240</td>\n      <td>180.420222</td>\n      <td>45.406983</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "        repeat_1      repeat_2      repeat_3          mean           std\n0   34429.679688  87992.460938  78462.750000  66961.630208  28573.583975\n1    5892.325195  23292.705078  46373.929688  25186.319987  20307.127040\n2    4111.504395   7904.031738  23883.576172  11966.370768  10493.362397\n3    3240.222900   6853.458984   9134.296875   6409.326253   2972.030857\n4    2621.375488   6083.708008   2640.574463   3781.885986   1993.459459\n5    2132.586426   5448.173828   1680.892456   3087.217570   2057.083542\n6    1746.095337   4906.398926   1601.384888   2751.293050   1867.778434\n7    1450.319580   4361.524414   1525.901489   2445.915161   1659.396657\n8    1231.878296   3906.026123   1459.434082   2199.112834   1482.602490\n9    1048.697266   3509.086670   1393.478516   1983.754150   1332.177916\n10    897.595764   3160.437012   1329.252441   1795.761739   1201.389122\n11    779.736755   2816.369385   1270.107300   1622.071147   1062.956739\n12    682.232422   2545.971680   1212.253906   1480.152669    960.316779\n13    604.918579   2300.545166   1160.678711   1355.380819    864.418338\n14    539.434204   2083.759033   1107.377197   1243.523478    781.112445\n15    488.897095   1888.843262   1049.864746   1142.535034    704.558837\n16    445.829132   1720.987671   1005.633240   1057.483348    639.158548\n17    409.947906   1565.088379    957.209961    977.415415    577.835247\n18    380.117981   1424.775146    909.978333    904.957153    522.346683\n19    354.205505   1302.506470    868.000977    841.570984    474.702631\n20    329.590881   1189.816528    826.797852    782.068420    431.853659\n21    310.167694   1090.537231    785.913757    728.872894    393.299375\n22    295.198975   1002.805664    749.684448    682.563029    358.546748\n23    275.650177    915.608154    713.345947    634.868093    327.117155\n24    261.562286    839.411255    681.874939    594.282827    298.716657\n25    248.770264    773.977905    645.910400    556.219523    273.850519\n26    240.974731    711.858093    614.805115    522.545980    248.629435\n27    228.133133    655.793701    589.280029    491.068954    230.124861\n28    218.904587    604.244812    554.309753    459.153051    209.553981\n29    210.797653    558.316162    529.224792    432.779536    192.791452\n30    205.262573    514.175781    502.533203    407.323853    175.087001\n31    197.601379    475.847168    479.925659    384.458069    161.835488\n32    191.639633    440.772766    455.054749    362.489049    148.132157\n33    186.395630    408.441833    433.439392    342.758952    135.990205\n34    183.430801    379.950897    413.666290    325.682663    124.341771\n35    178.420654    352.559296    394.599213    308.526388    114.618781\n36    173.795105    328.575836    376.928619    293.099854    106.111805\n37    170.126541    306.288361    360.769836    279.061579     98.194652\n38    168.455841    286.742523    344.346924    266.515096     89.673184\n39    167.529251    268.344849    330.098328    255.324142     82.062965\n40    162.989899    251.906570    317.065826    243.987432     77.342630\n41    160.831726    238.028671    305.306244    234.722214     72.293991\n42    159.927429    221.084991    293.756012    224.922811     66.996784\n43    157.458221    209.059341    282.205414    216.240992     62.682913\n44    156.768509    197.887955    272.871155    209.175873     58.868658\n45    153.954926    189.180740    267.584778    203.573481     58.166133\n46    154.328766    179.484406    257.619568    197.144246     53.862324\n47    152.048874    172.488205    246.580948    190.372676     49.739009\n48    153.034866    164.858536    239.323318    185.738907     46.780516\n49    150.041107    158.601318    232.618240    180.420222     45.406983"
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"item34\"></a>"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## B. Normalize the data"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "predictors_norm = (predictors - predictors.mean()) / predictors.std()\npredictors_norm.head()",
            "execution_count": 16,
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.476712</td>\n      <td>-0.856472</td>\n      <td>-0.846733</td>\n      <td>-0.916319</td>\n      <td>-0.620147</td>\n      <td>0.862735</td>\n      <td>-1.217079</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.476712</td>\n      <td>-0.856472</td>\n      <td>-0.846733</td>\n      <td>-0.916319</td>\n      <td>-0.620147</td>\n      <td>1.055651</td>\n      <td>-1.217079</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.491187</td>\n      <td>0.795140</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>3.551340</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.491187</td>\n      <td>0.795140</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>5.055221</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.790075</td>\n      <td>0.678079</td>\n      <td>-0.846733</td>\n      <td>0.488555</td>\n      <td>-1.038638</td>\n      <td>0.070492</td>\n      <td>0.647569</td>\n      <td>4.976069</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "     Cement  Blast Furnace Slag   Fly Ash     Water  Superplasticizer  \\\n0  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n1  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n2  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n3  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n4 -0.790075            0.678079 -0.846733  0.488555         -1.038638   \n\n   Coarse Aggregate  Fine Aggregate       Age  \n0          0.862735       -1.217079 -0.279597  \n1          1.055651       -1.217079 -0.279597  \n2         -0.526262       -2.239829  3.551340  \n3         -0.526262       -2.239829  5.055221  \n4          0.070492        0.647569  4.976069  "
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Let's save the number of predictors to *n_cols* since we will need this number when building our network."
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "n_cols = predictors_norm.shape[1] # number of predictors\nn_cols",
            "execution_count": 17,
            "outputs": [
                {
                    "data": {
                        "text/plain": "8"
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "# fit the model\ndf_norm = pd.DataFrame()\nrepeats = 3\nfor repeat in range(repeats):\n    model = regression_model()\n    history_callback = model.fit(predictors_norm, target, validation_split=0.3, epochs=50, verbose=2)\n    \n    layer_name = 'repeat_{}'.format(repeat + 1) # otherwise give the layer a number\n    df_norm[layer_name] = history_callback.history[\"loss\"]\n    \n    if repeat == repeats-1 :\n        df_norm['mean'] = df_norm.mean(axis=1)\n        df_norm['std'] = df_norm.iloc[:,0:repeats].std(axis=1)",
            "execution_count": 18,
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/50\n23/23 - 0s - loss: 1654.9481 - val_loss: 1192.0492\nEpoch 2/50\n23/23 - 0s - loss: 1636.5280 - val_loss: 1178.3784\nEpoch 3/50\n23/23 - 0s - loss: 1617.5645 - val_loss: 1164.4292\nEpoch 4/50\n23/23 - 0s - loss: 1597.8940 - val_loss: 1149.9236\nEpoch 5/50\n23/23 - 0s - loss: 1577.2311 - val_loss: 1135.2983\nEpoch 6/50\n23/23 - 0s - loss: 1555.7318 - val_loss: 1119.4343\nEpoch 7/50\n23/23 - 0s - loss: 1533.1434 - val_loss: 1103.4462\nEpoch 8/50\n23/23 - 0s - loss: 1509.5677 - val_loss: 1086.7029\nEpoch 9/50\n23/23 - 0s - loss: 1484.5310 - val_loss: 1069.0197\nEpoch 10/50\n23/23 - 0s - loss: 1458.1587 - val_loss: 1051.2563\nEpoch 11/50\n23/23 - 0s - loss: 1430.5098 - val_loss: 1032.3098\nEpoch 12/50\n23/23 - 0s - loss: 1401.2866 - val_loss: 1013.4681\nEpoch 13/50\n23/23 - 0s - loss: 1371.4725 - val_loss: 993.5945\nEpoch 14/50\n23/23 - 0s - loss: 1339.7592 - val_loss: 973.2398\nEpoch 15/50\n23/23 - 0s - loss: 1307.5854 - val_loss: 952.3077\nEpoch 16/50\n23/23 - 0s - loss: 1273.6418 - val_loss: 931.2615\nEpoch 17/50\n23/23 - 0s - loss: 1238.3954 - val_loss: 909.8271\nEpoch 18/50\n23/23 - 0s - loss: 1202.6565 - val_loss: 887.5844\nEpoch 19/50\n23/23 - 0s - loss: 1166.2980 - val_loss: 865.0347\nEpoch 20/50\n23/23 - 0s - loss: 1129.4047 - val_loss: 842.6413\nEpoch 21/50\n23/23 - 0s - loss: 1092.1040 - val_loss: 820.6016\nEpoch 22/50\n23/23 - 0s - loss: 1054.4886 - val_loss: 797.9078\nEpoch 23/50\n23/23 - 0s - loss: 1017.3199 - val_loss: 775.4439\nEpoch 24/50\n23/23 - 0s - loss: 979.5012 - val_loss: 753.0739\nEpoch 25/50\n23/23 - 0s - loss: 942.4661 - val_loss: 730.9361\nEpoch 26/50\n23/23 - 0s - loss: 906.0704 - val_loss: 708.9983\nEpoch 27/50\n23/23 - 0s - loss: 869.6778 - val_loss: 687.9401\nEpoch 28/50\n23/23 - 0s - loss: 834.1172 - val_loss: 666.5069\nEpoch 29/50\n23/23 - 0s - loss: 799.1108 - val_loss: 645.9628\nEpoch 30/50\n23/23 - 0s - loss: 765.2023 - val_loss: 625.3801\nEpoch 31/50\n23/23 - 0s - loss: 731.7970 - val_loss: 605.7198\nEpoch 32/50\n23/23 - 0s - loss: 699.4741 - val_loss: 586.7246\nEpoch 33/50\n23/23 - 0s - loss: 668.1272 - val_loss: 567.8611\nEpoch 34/50\n23/23 - 0s - loss: 638.7839 - val_loss: 548.9835\nEpoch 35/50\n23/23 - 0s - loss: 609.6212 - val_loss: 532.0228\nEpoch 36/50\n23/23 - 0s - loss: 582.0786 - val_loss: 514.7864\nEpoch 37/50\n23/23 - 0s - loss: 555.9583 - val_loss: 499.0165\nEpoch 38/50\n23/23 - 0s - loss: 530.7287 - val_loss: 483.5605\nEpoch 39/50\n23/23 - 0s - loss: 507.0482 - val_loss: 468.6151\nEpoch 40/50\n23/23 - 0s - loss: 484.5548 - val_loss: 454.0119\nEpoch 41/50\n23/23 - 0s - loss: 463.1729 - val_loss: 440.2919\nEpoch 42/50\n23/23 - 0s - loss: 442.9043 - val_loss: 426.7274\nEpoch 43/50\n23/23 - 0s - loss: 424.2694 - val_loss: 414.3264\nEpoch 44/50\n23/23 - 0s - loss: 406.4784 - val_loss: 401.9012\nEpoch 45/50\n23/23 - 0s - loss: 389.8000 - val_loss: 390.7114\nEpoch 46/50\n23/23 - 0s - loss: 374.5732 - val_loss: 379.1497\nEpoch 47/50\n23/23 - 0s - loss: 359.7195 - val_loss: 369.4135\nEpoch 48/50\n23/23 - 0s - loss: 346.8900 - val_loss: 359.1871\nEpoch 49/50\n23/23 - 0s - loss: 334.0374 - val_loss: 349.8901\nEpoch 50/50\n23/23 - 0s - loss: 322.7260 - val_loss: 341.1391\nEpoch 1/50\n23/23 - 0s - loss: 1739.5011 - val_loss: 1258.8380\nEpoch 2/50\n23/23 - 0s - loss: 1722.4591 - val_loss: 1246.7131\nEpoch 3/50\n23/23 - 0s - loss: 1706.0317 - val_loss: 1235.0834\nEpoch 4/50\n23/23 - 0s - loss: 1690.1451 - val_loss: 1223.6049\nEpoch 5/50\n23/23 - 0s - loss: 1674.5549 - val_loss: 1211.9524\nEpoch 6/50\n23/23 - 0s - loss: 1658.7322 - val_loss: 1200.4517\nEpoch 7/50\n23/23 - 0s - loss: 1643.0504 - val_loss: 1188.6255\nEpoch 8/50\n23/23 - 0s - loss: 1626.6598 - val_loss: 1176.6530\nEpoch 9/50\n23/23 - 0s - loss: 1610.0950 - val_loss: 1164.2427\nEpoch 10/50\n23/23 - 0s - loss: 1592.4712 - val_loss: 1151.6271\nEpoch 11/50\n23/23 - 0s - loss: 1574.7233 - val_loss: 1138.0970\nEpoch 12/50\n23/23 - 0s - loss: 1555.4329 - val_loss: 1124.1613\nEpoch 13/50\n23/23 - 0s - loss: 1535.5719 - val_loss: 1109.1851\nEpoch 14/50\n23/23 - 0s - loss: 1514.2313 - val_loss: 1093.7120\nEpoch 15/50\n23/23 - 0s - loss: 1492.0438 - val_loss: 1077.2216\nEpoch 16/50\n23/23 - 0s - loss: 1468.2657 - val_loss: 1060.3511\nEpoch 17/50\n23/23 - 0s - loss: 1443.5055 - val_loss: 1042.5309\nEpoch 18/50\n23/23 - 0s - loss: 1417.3000 - val_loss: 1023.7936\nEpoch 19/50\n23/23 - 0s - loss: 1389.9878 - val_loss: 1003.9039\nEpoch 20/50\n23/23 - 0s - loss: 1360.9642 - val_loss: 983.7100\nEpoch 21/50\n23/23 - 0s - loss: 1330.9115 - val_loss: 962.6612\nEpoch 22/50\n23/23 - 0s - loss: 1299.9076 - val_loss: 941.0916\nEpoch 23/50\n23/23 - 0s - loss: 1267.6558 - val_loss: 919.1997\nEpoch 24/50\n23/23 - 0s - loss: 1234.5419 - val_loss: 896.4500\nEpoch 25/50\n23/23 - 0s - loss: 1200.3877 - val_loss: 873.7834\nEpoch 26/50\n23/23 - 0s - loss: 1165.1260 - val_loss: 850.8414\nEpoch 27/50\n23/23 - 0s - loss: 1129.0870 - val_loss: 827.3716\nEpoch 28/50\n23/23 - 0s - loss: 1091.8607 - val_loss: 804.2305\nEpoch 29/50\n23/23 - 0s - loss: 1054.2585 - val_loss: 780.7374\nEpoch 30/50\n23/23 - 0s - loss: 1016.2675 - val_loss: 757.1918\nEpoch 31/50\n23/23 - 0s - loss: 978.1991 - val_loss: 734.1809\nEpoch 32/50\n23/23 - 0s - loss: 939.8015 - val_loss: 711.2876\nEpoch 33/50\n23/23 - 0s - loss: 902.1954 - val_loss: 687.7430\nEpoch 34/50\n23/23 - 0s - loss: 864.3269 - val_loss: 665.9468\nEpoch 35/50\n23/23 - 0s - loss: 827.3505 - val_loss: 643.4521\nEpoch 36/50\n23/23 - 0s - loss: 790.7789 - val_loss: 621.9134\nEpoch 37/50\n23/23 - 0s - loss: 755.2264 - val_loss: 601.2763\nEpoch 38/50\n23/23 - 0s - loss: 720.3897 - val_loss: 580.3714\nEpoch 39/50\n23/23 - 0s - loss: 686.6683 - val_loss: 560.4023\nEpoch 40/50\n23/23 - 0s - loss: 653.9271 - val_loss: 540.9175\nEpoch 41/50\n23/23 - 0s - loss: 622.2796 - val_loss: 522.1960\nEpoch 42/50\n23/23 - 0s - loss: 591.9969 - val_loss: 503.8493\nEpoch 43/50\n23/23 - 0s - loss: 562.2758 - val_loss: 485.8659\nEpoch 44/50\n23/23 - 0s - loss: 534.5703 - val_loss: 468.7585\nEpoch 45/50\n23/23 - 0s - loss: 507.7634 - val_loss: 451.9791\nEpoch 46/50\n23/23 - 0s - loss: 482.0453 - val_loss: 435.8777\nEpoch 47/50\n23/23 - 0s - loss: 458.1802 - val_loss: 420.1207\nEpoch 48/50\n23/23 - 0s - loss: 435.3075 - val_loss: 405.3166\nEpoch 49/50\n23/23 - 0s - loss: 413.6247 - val_loss: 391.3559\nEpoch 50/50\n23/23 - 0s - loss: 393.8093 - val_loss: 377.8905\nEpoch 1/50\n23/23 - 0s - loss: 1690.8408 - val_loss: 1246.0226\nEpoch 2/50\n23/23 - 0s - loss: 1667.5586 - val_loss: 1231.8955\nEpoch 3/50\n23/23 - 0s - loss: 1644.1910 - val_loss: 1218.3020\nEpoch 4/50\n23/23 - 0s - loss: 1620.9781 - val_loss: 1204.7379\nEpoch 5/50\n23/23 - 0s - loss: 1597.7885 - val_loss: 1191.7008\nEpoch 6/50\n23/23 - 0s - loss: 1574.4426 - val_loss: 1178.3271\nEpoch 7/50\n23/23 - 0s - loss: 1550.3337 - val_loss: 1165.3418\nEpoch 8/50\n23/23 - 0s - loss: 1526.3058 - val_loss: 1151.7573\nEpoch 9/50\n23/23 - 0s - loss: 1501.1425 - val_loss: 1138.2745\nEpoch 10/50\n23/23 - 0s - loss: 1475.4460 - val_loss: 1124.6393\nEpoch 11/50\n23/23 - 0s - loss: 1449.0177 - val_loss: 1110.8903\nEpoch 12/50\n23/23 - 0s - loss: 1421.6328 - val_loss: 1096.6954\nEpoch 13/50\n23/23 - 0s - loss: 1393.1635 - val_loss: 1082.1777\nEpoch 14/50\n23/23 - 0s - loss: 1364.1129 - val_loss: 1067.3146\nEpoch 15/50\n23/23 - 0s - loss: 1333.5082 - val_loss: 1051.9756\nEpoch 16/50\n23/23 - 0s - loss: 1302.2036 - val_loss: 1036.2976\nEpoch 17/50\n23/23 - 0s - loss: 1270.4701 - val_loss: 1020.2723\nEpoch 18/50\n23/23 - 0s - loss: 1237.7487 - val_loss: 1003.7794\nEpoch 19/50\n23/23 - 0s - loss: 1204.3600 - val_loss: 987.1409\nEpoch 20/50\n23/23 - 0s - loss: 1170.8738 - val_loss: 969.8713\nEpoch 21/50\n23/23 - 0s - loss: 1136.6346 - val_loss: 952.4438\nEpoch 22/50\n23/23 - 0s - loss: 1102.6237 - val_loss: 934.6872\nEpoch 23/50\n23/23 - 0s - loss: 1068.1154 - val_loss: 916.4473\nEpoch 24/50\n23/23 - 0s - loss: 1033.8213 - val_loss: 898.2095\nEpoch 25/50\n23/23 - 0s - loss: 999.4364 - val_loss: 880.0662\nEpoch 26/50\n23/23 - 0s - loss: 965.3625 - val_loss: 861.7231\nEpoch 27/50\n23/23 - 0s - loss: 931.6774 - val_loss: 843.3898\nEpoch 28/50\n23/23 - 0s - loss: 898.8139 - val_loss: 824.8901\nEpoch 29/50\n23/23 - 0s - loss: 866.1828 - val_loss: 806.4138\nEpoch 30/50\n23/23 - 0s - loss: 834.4280 - val_loss: 788.3998\nEpoch 31/50\n23/23 - 0s - loss: 803.3336 - val_loss: 770.4402\nEpoch 32/50\n23/23 - 0s - loss: 773.6223 - val_loss: 752.4039\nEpoch 33/50\n23/23 - 0s - loss: 743.6691 - val_loss: 735.1856\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 34/50\n23/23 - 0s - loss: 715.5382 - val_loss: 717.8964\nEpoch 35/50\n23/23 - 0s - loss: 687.9860 - val_loss: 700.2939\nEpoch 36/50\n23/23 - 0s - loss: 661.5745 - val_loss: 683.4928\nEpoch 37/50\n23/23 - 0s - loss: 635.9660 - val_loss: 666.1486\nEpoch 38/50\n23/23 - 0s - loss: 611.2286 - val_loss: 649.1473\nEpoch 39/50\n23/23 - 0s - loss: 587.5609 - val_loss: 632.4630\nEpoch 40/50\n23/23 - 0s - loss: 564.5193 - val_loss: 616.5649\nEpoch 41/50\n23/23 - 0s - loss: 542.6450 - val_loss: 600.1851\nEpoch 42/50\n23/23 - 0s - loss: 521.8179 - val_loss: 584.4454\nEpoch 43/50\n23/23 - 0s - loss: 501.9438 - val_loss: 569.1311\nEpoch 44/50\n23/23 - 0s - loss: 482.8821 - val_loss: 553.8461\nEpoch 45/50\n23/23 - 0s - loss: 465.0396 - val_loss: 538.9094\nEpoch 46/50\n23/23 - 0s - loss: 447.8897 - val_loss: 524.6554\nEpoch 47/50\n23/23 - 0s - loss: 431.5663 - val_loss: 510.8921\nEpoch 48/50\n23/23 - 0s - loss: 416.3226 - val_loss: 497.4547\nEpoch 49/50\n23/23 - 0s - loss: 402.0028 - val_loss: 484.2351\nEpoch 50/50\n23/23 - 0s - loss: 388.6953 - val_loss: 471.6549\n"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Report the mean and the standard deviation of the mean squared errors."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "print(df_norm.shape)\ndf_norm",
            "execution_count": 19,
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "(50, 5)\n"
                },
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>repeat_1</th>\n      <th>repeat_2</th>\n      <th>repeat_3</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1654.948120</td>\n      <td>1739.501099</td>\n      <td>1690.840820</td>\n      <td>1695.096680</td>\n      <td>42.436845</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1636.527954</td>\n      <td>1722.459106</td>\n      <td>1667.558594</td>\n      <td>1675.515218</td>\n      <td>43.514614</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1617.564453</td>\n      <td>1706.031738</td>\n      <td>1644.191040</td>\n      <td>1655.929077</td>\n      <td>45.386686</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1597.894043</td>\n      <td>1690.145142</td>\n      <td>1620.978149</td>\n      <td>1636.339111</td>\n      <td>48.005579</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1577.231079</td>\n      <td>1674.554932</td>\n      <td>1597.788452</td>\n      <td>1616.524821</td>\n      <td>51.295923</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1555.731812</td>\n      <td>1658.732178</td>\n      <td>1574.442627</td>\n      <td>1596.302205</td>\n      <td>54.869388</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1533.143433</td>\n      <td>1643.050415</td>\n      <td>1550.333740</td>\n      <td>1575.509196</td>\n      <td>59.120545</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1509.567749</td>\n      <td>1626.659790</td>\n      <td>1526.305786</td>\n      <td>1554.177775</td>\n      <td>63.326711</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1484.531006</td>\n      <td>1610.094971</td>\n      <td>1501.142456</td>\n      <td>1531.922811</td>\n      <td>68.206671</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1458.158691</td>\n      <td>1592.471191</td>\n      <td>1475.446045</td>\n      <td>1508.691976</td>\n      <td>73.067988</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1430.509766</td>\n      <td>1574.723267</td>\n      <td>1449.017700</td>\n      <td>1484.750244</td>\n      <td>78.466518</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1401.286621</td>\n      <td>1555.432861</td>\n      <td>1421.632812</td>\n      <td>1459.450765</td>\n      <td>83.743143</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1371.472534</td>\n      <td>1535.571899</td>\n      <td>1393.163452</td>\n      <td>1433.402629</td>\n      <td>89.143390</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1339.759155</td>\n      <td>1514.231323</td>\n      <td>1364.112915</td>\n      <td>1406.034465</td>\n      <td>94.489135</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1307.585449</td>\n      <td>1492.043823</td>\n      <td>1333.508179</td>\n      <td>1377.712484</td>\n      <td>99.858592</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1273.641846</td>\n      <td>1468.265747</td>\n      <td>1302.203613</td>\n      <td>1348.037069</td>\n      <td>105.095885</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1238.395386</td>\n      <td>1443.505493</td>\n      <td>1270.470093</td>\n      <td>1317.456991</td>\n      <td>110.332975</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1202.656494</td>\n      <td>1417.300049</td>\n      <td>1237.748657</td>\n      <td>1285.901733</td>\n      <td>115.139059</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1166.297974</td>\n      <td>1389.987793</td>\n      <td>1204.359985</td>\n      <td>1253.548584</td>\n      <td>119.682591</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1129.404663</td>\n      <td>1360.964233</td>\n      <td>1170.873779</td>\n      <td>1220.414225</td>\n      <td>123.473278</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>1092.104004</td>\n      <td>1330.911499</td>\n      <td>1136.634644</td>\n      <td>1186.550049</td>\n      <td>126.987856</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1054.488647</td>\n      <td>1299.907593</td>\n      <td>1102.623657</td>\n      <td>1152.339966</td>\n      <td>130.043832</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1017.319946</td>\n      <td>1267.655762</td>\n      <td>1068.115356</td>\n      <td>1117.697021</td>\n      <td>132.328233</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>979.501221</td>\n      <td>1234.541870</td>\n      <td>1033.821289</td>\n      <td>1082.621460</td>\n      <td>134.341080</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>942.466125</td>\n      <td>1200.387695</td>\n      <td>999.436401</td>\n      <td>1047.430074</td>\n      <td>135.493278</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>906.070374</td>\n      <td>1165.125977</td>\n      <td>965.362488</td>\n      <td>1012.186279</td>\n      <td>135.726939</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>869.677795</td>\n      <td>1129.087036</td>\n      <td>931.677368</td>\n      <td>976.814067</td>\n      <td>135.466895</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>834.117188</td>\n      <td>1091.860718</td>\n      <td>898.813904</td>\n      <td>941.597270</td>\n      <td>134.092297</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>799.110840</td>\n      <td>1054.258545</td>\n      <td>866.182800</td>\n      <td>906.517395</td>\n      <td>132.269602</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>765.202332</td>\n      <td>1016.267517</td>\n      <td>834.427979</td>\n      <td>871.965942</td>\n      <td>129.673652</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>731.796997</td>\n      <td>978.199097</td>\n      <td>803.333557</td>\n      <td>837.776550</td>\n      <td>126.760556</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>699.474060</td>\n      <td>939.801514</td>\n      <td>773.622253</td>\n      <td>804.299276</td>\n      <td>123.065556</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>668.127197</td>\n      <td>902.195374</td>\n      <td>743.669128</td>\n      <td>771.330566</td>\n      <td>119.460638</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>638.783875</td>\n      <td>864.326904</td>\n      <td>715.538208</td>\n      <td>739.549662</td>\n      <td>114.672695</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>609.621216</td>\n      <td>827.350464</td>\n      <td>687.985962</td>\n      <td>708.319214</td>\n      <td>110.279587</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>582.078613</td>\n      <td>790.778931</td>\n      <td>661.574524</td>\n      <td>678.144023</td>\n      <td>105.332174</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>555.958313</td>\n      <td>755.226440</td>\n      <td>635.966003</td>\n      <td>649.050252</td>\n      <td>100.276342</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>530.728699</td>\n      <td>720.389709</td>\n      <td>611.228577</td>\n      <td>620.782328</td>\n      <td>95.190758</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>507.048157</td>\n      <td>686.668335</td>\n      <td>587.560913</td>\n      <td>593.759135</td>\n      <td>89.970359</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>484.554779</td>\n      <td>653.927063</td>\n      <td>564.519348</td>\n      <td>567.667063</td>\n      <td>84.730005</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>463.172943</td>\n      <td>622.279602</td>\n      <td>542.645020</td>\n      <td>542.699188</td>\n      <td>79.553343</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>442.904297</td>\n      <td>591.996948</td>\n      <td>521.817871</td>\n      <td>518.906372</td>\n      <td>74.588956</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>424.269409</td>\n      <td>562.275757</td>\n      <td>501.943817</td>\n      <td>496.162994</td>\n      <td>69.184546</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>406.478363</td>\n      <td>534.570312</td>\n      <td>482.882141</td>\n      <td>474.643606</td>\n      <td>64.442160</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>389.799988</td>\n      <td>507.763428</td>\n      <td>465.039612</td>\n      <td>454.201009</td>\n      <td>59.723946</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>374.573212</td>\n      <td>482.045349</td>\n      <td>447.889679</td>\n      <td>434.836080</td>\n      <td>54.912316</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>359.719513</td>\n      <td>458.180176</td>\n      <td>431.566345</td>\n      <td>416.488678</td>\n      <td>50.932579</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>346.889984</td>\n      <td>435.307526</td>\n      <td>416.322632</td>\n      <td>399.506714</td>\n      <td>46.545641</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>334.037354</td>\n      <td>413.624725</td>\n      <td>402.002777</td>\n      <td>383.221619</td>\n      <td>42.989375</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>322.725952</td>\n      <td>393.809296</td>\n      <td>388.695251</td>\n      <td>368.410166</td>\n      <td>39.646235</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "       repeat_1     repeat_2     repeat_3         mean         std\n0   1654.948120  1739.501099  1690.840820  1695.096680   42.436845\n1   1636.527954  1722.459106  1667.558594  1675.515218   43.514614\n2   1617.564453  1706.031738  1644.191040  1655.929077   45.386686\n3   1597.894043  1690.145142  1620.978149  1636.339111   48.005579\n4   1577.231079  1674.554932  1597.788452  1616.524821   51.295923\n5   1555.731812  1658.732178  1574.442627  1596.302205   54.869388\n6   1533.143433  1643.050415  1550.333740  1575.509196   59.120545\n7   1509.567749  1626.659790  1526.305786  1554.177775   63.326711\n8   1484.531006  1610.094971  1501.142456  1531.922811   68.206671\n9   1458.158691  1592.471191  1475.446045  1508.691976   73.067988\n10  1430.509766  1574.723267  1449.017700  1484.750244   78.466518\n11  1401.286621  1555.432861  1421.632812  1459.450765   83.743143\n12  1371.472534  1535.571899  1393.163452  1433.402629   89.143390\n13  1339.759155  1514.231323  1364.112915  1406.034465   94.489135\n14  1307.585449  1492.043823  1333.508179  1377.712484   99.858592\n15  1273.641846  1468.265747  1302.203613  1348.037069  105.095885\n16  1238.395386  1443.505493  1270.470093  1317.456991  110.332975\n17  1202.656494  1417.300049  1237.748657  1285.901733  115.139059\n18  1166.297974  1389.987793  1204.359985  1253.548584  119.682591\n19  1129.404663  1360.964233  1170.873779  1220.414225  123.473278\n20  1092.104004  1330.911499  1136.634644  1186.550049  126.987856\n21  1054.488647  1299.907593  1102.623657  1152.339966  130.043832\n22  1017.319946  1267.655762  1068.115356  1117.697021  132.328233\n23   979.501221  1234.541870  1033.821289  1082.621460  134.341080\n24   942.466125  1200.387695   999.436401  1047.430074  135.493278\n25   906.070374  1165.125977   965.362488  1012.186279  135.726939\n26   869.677795  1129.087036   931.677368   976.814067  135.466895\n27   834.117188  1091.860718   898.813904   941.597270  134.092297\n28   799.110840  1054.258545   866.182800   906.517395  132.269602\n29   765.202332  1016.267517   834.427979   871.965942  129.673652\n30   731.796997   978.199097   803.333557   837.776550  126.760556\n31   699.474060   939.801514   773.622253   804.299276  123.065556\n32   668.127197   902.195374   743.669128   771.330566  119.460638\n33   638.783875   864.326904   715.538208   739.549662  114.672695\n34   609.621216   827.350464   687.985962   708.319214  110.279587\n35   582.078613   790.778931   661.574524   678.144023  105.332174\n36   555.958313   755.226440   635.966003   649.050252  100.276342\n37   530.728699   720.389709   611.228577   620.782328   95.190758\n38   507.048157   686.668335   587.560913   593.759135   89.970359\n39   484.554779   653.927063   564.519348   567.667063   84.730005\n40   463.172943   622.279602   542.645020   542.699188   79.553343\n41   442.904297   591.996948   521.817871   518.906372   74.588956\n42   424.269409   562.275757   501.943817   496.162994   69.184546\n43   406.478363   534.570312   482.882141   474.643606   64.442160\n44   389.799988   507.763428   465.039612   454.201009   59.723946\n45   374.573212   482.045349   447.889679   434.836080   54.912316\n46   359.719513   458.180176   431.566345   416.488678   50.932579\n47   346.889984   435.307526   416.322632   399.506714   46.545641\n48   334.037354   413.624725   402.002777   383.221619   42.989375\n49   322.725952   393.809296   388.695251   368.410166   39.646235"
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### How does the mean of the mean squared errors compare to that from Step A?"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "In step A the mean square error in its first iterations was much higher compared to step B, however, at the end of the 50 times, step A had a smaller error, this implies that in the end the non-normalized data give better results. results in this case."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"item35\"></a>"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## C. Increate the number of epochs"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Using 100 epochs this time for training"
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "# fit the model\ndf_norm = pd.DataFrame()\nrepeats = 3\nfor repeat in range(repeats):\n    model = regression_model()\n    history_callback = model.fit(predictors_norm, target, validation_split=0.3, epochs=100, verbose=2)\n    \n    layer_name = 'repeat_{}'.format(repeat + 1) # otherwise give the layer a number\n    df_norm[layer_name] = history_callback.history[\"loss\"]\n    \n    if repeat == repeats-1 :\n        df_norm['mean'] = df_norm.mean(axis=1)\n        df_norm['std'] = df_norm.iloc[:,0:repeats].std(axis=1)",
            "execution_count": 20,
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/100\n23/23 - 0s - loss: 1667.7979 - val_loss: 1183.5328\nEpoch 2/100\n23/23 - 0s - loss: 1647.6486 - val_loss: 1168.8748\nEpoch 3/100\n23/23 - 0s - loss: 1627.2573 - val_loss: 1154.1864\nEpoch 4/100\n23/23 - 0s - loss: 1606.3318 - val_loss: 1139.0001\nEpoch 5/100\n23/23 - 0s - loss: 1584.5892 - val_loss: 1123.7074\nEpoch 6/100\n23/23 - 0s - loss: 1562.0723 - val_loss: 1108.2302\nEpoch 7/100\n23/23 - 0s - loss: 1538.7389 - val_loss: 1092.1567\nEpoch 8/100\n23/23 - 0s - loss: 1514.5055 - val_loss: 1075.5554\nEpoch 9/100\n23/23 - 0s - loss: 1489.2413 - val_loss: 1058.9531\nEpoch 10/100\n23/23 - 0s - loss: 1462.8254 - val_loss: 1041.8046\nEpoch 11/100\n23/23 - 0s - loss: 1435.5707 - val_loss: 1024.3715\nEpoch 12/100\n23/23 - 0s - loss: 1407.3275 - val_loss: 1005.8859\nEpoch 13/100\n23/23 - 0s - loss: 1378.0555 - val_loss: 987.4186\nEpoch 14/100\n23/23 - 0s - loss: 1348.4230 - val_loss: 968.5565\nEpoch 15/100\n23/23 - 0s - loss: 1317.6980 - val_loss: 949.2562\nEpoch 16/100\n23/23 - 0s - loss: 1287.2133 - val_loss: 929.6185\nEpoch 17/100\n23/23 - 0s - loss: 1254.6989 - val_loss: 910.2964\nEpoch 18/100\n23/23 - 0s - loss: 1222.2522 - val_loss: 890.3318\nEpoch 19/100\n23/23 - 0s - loss: 1189.2610 - val_loss: 870.2044\nEpoch 20/100\n23/23 - 0s - loss: 1155.3728 - val_loss: 849.9997\nEpoch 21/100\n23/23 - 0s - loss: 1121.4586 - val_loss: 829.9584\nEpoch 22/100\n23/23 - 0s - loss: 1086.6102 - val_loss: 810.2208\nEpoch 23/100\n23/23 - 0s - loss: 1052.0544 - val_loss: 789.4734\nEpoch 24/100\n23/23 - 0s - loss: 1017.4159 - val_loss: 768.9061\nEpoch 25/100\n23/23 - 0s - loss: 982.4228 - val_loss: 748.3879\nEpoch 26/100\n23/23 - 0s - loss: 947.8655 - val_loss: 728.0425\nEpoch 27/100\n23/23 - 0s - loss: 913.5549 - val_loss: 708.6738\nEpoch 28/100\n23/23 - 0s - loss: 880.3398 - val_loss: 687.9365\nEpoch 29/100\n23/23 - 0s - loss: 846.5579 - val_loss: 668.0769\nEpoch 30/100\n23/23 - 0s - loss: 814.1382 - val_loss: 648.8219\nEpoch 31/100\n23/23 - 0s - loss: 782.3048 - val_loss: 629.6000\nEpoch 32/100\n23/23 - 0s - loss: 751.4247 - val_loss: 610.4923\nEpoch 33/100\n23/23 - 0s - loss: 721.0282 - val_loss: 592.1422\nEpoch 34/100\n23/23 - 0s - loss: 691.5506 - val_loss: 573.7881\nEpoch 35/100\n23/23 - 0s - loss: 662.9025 - val_loss: 555.5988\nEpoch 36/100\n23/23 - 0s - loss: 634.9332 - val_loss: 538.4185\nEpoch 37/100\n23/23 - 0s - loss: 608.2668 - val_loss: 521.3489\nEpoch 38/100\n23/23 - 0s - loss: 582.4259 - val_loss: 504.1786\nEpoch 39/100\n23/23 - 0s - loss: 557.4911 - val_loss: 487.7660\nEpoch 40/100\n23/23 - 0s - loss: 533.4116 - val_loss: 472.2232\nEpoch 41/100\n23/23 - 0s - loss: 510.6969 - val_loss: 456.4666\nEpoch 42/100\n23/23 - 0s - loss: 488.8443 - val_loss: 441.3051\nEpoch 43/100\n23/23 - 0s - loss: 467.9304 - val_loss: 427.0516\nEpoch 44/100\n23/23 - 0s - loss: 447.8018 - val_loss: 413.0983\nEpoch 45/100\n23/23 - 0s - loss: 429.1661 - val_loss: 399.5015\nEpoch 46/100\n23/23 - 0s - loss: 410.9882 - val_loss: 386.4842\nEpoch 47/100\n23/23 - 0s - loss: 393.9884 - val_loss: 374.8217\nEpoch 48/100\n23/23 - 0s - loss: 378.3044 - val_loss: 362.7019\nEpoch 49/100\n23/23 - 0s - loss: 363.2925 - val_loss: 352.1092\nEpoch 50/100\n23/23 - 0s - loss: 349.5919 - val_loss: 341.4979\nEpoch 51/100\n23/23 - 0s - loss: 336.7826 - val_loss: 331.3664\nEpoch 52/100\n23/23 - 0s - loss: 324.6066 - val_loss: 322.3351\nEpoch 53/100\n23/23 - 0s - loss: 313.7145 - val_loss: 313.2057\nEpoch 54/100\n23/23 - 0s - loss: 303.3766 - val_loss: 305.5496\nEpoch 55/100\n23/23 - 0s - loss: 294.1201 - val_loss: 297.5706\nEpoch 56/100\n23/23 - 0s - loss: 285.3287 - val_loss: 291.1163\nEpoch 57/100\n23/23 - 0s - loss: 277.4195 - val_loss: 284.1166\nEpoch 58/100\n23/23 - 0s - loss: 270.0723 - val_loss: 278.1926\nEpoch 59/100\n23/23 - 0s - loss: 263.6275 - val_loss: 272.6374\nEpoch 60/100\n23/23 - 0s - loss: 257.5216 - val_loss: 267.6668\nEpoch 61/100\n23/23 - 0s - loss: 252.1399 - val_loss: 262.6025\nEpoch 62/100\n23/23 - 0s - loss: 247.1970 - val_loss: 258.9810\nEpoch 63/100\n23/23 - 0s - loss: 242.6657 - val_loss: 254.6934\nEpoch 64/100\n23/23 - 0s - loss: 238.5390 - val_loss: 250.9643\nEpoch 65/100\n23/23 - 0s - loss: 234.7027 - val_loss: 247.6675\nEpoch 66/100\n23/23 - 0s - loss: 231.2099 - val_loss: 244.6124\nEpoch 67/100\n23/23 - 0s - loss: 227.9852 - val_loss: 241.9006\nEpoch 68/100\n23/23 - 0s - loss: 225.0259 - val_loss: 238.6990\nEpoch 69/100\n23/23 - 0s - loss: 222.2108 - val_loss: 236.1507\nEpoch 70/100\n23/23 - 0s - loss: 219.7803 - val_loss: 233.9860\nEpoch 71/100\n23/23 - 0s - loss: 217.3604 - val_loss: 231.5819\nEpoch 72/100\n23/23 - 0s - loss: 215.1905 - val_loss: 229.9088\nEpoch 73/100\n23/23 - 0s - loss: 213.1630 - val_loss: 227.9671\nEpoch 74/100\n23/23 - 0s - loss: 211.0755 - val_loss: 225.7610\nEpoch 75/100\n23/23 - 0s - loss: 209.1499 - val_loss: 224.2722\nEpoch 76/100\n23/23 - 0s - loss: 207.3105 - val_loss: 222.3717\nEpoch 77/100\n23/23 - 0s - loss: 205.5259 - val_loss: 220.5445\nEpoch 78/100\n23/23 - 0s - loss: 203.7783 - val_loss: 219.3846\nEpoch 79/100\n23/23 - 0s - loss: 202.0207 - val_loss: 217.5508\nEpoch 80/100\n23/23 - 0s - loss: 200.4173 - val_loss: 216.4425\nEpoch 81/100\n23/23 - 0s - loss: 198.7705 - val_loss: 214.9397\nEpoch 82/100\n23/23 - 0s - loss: 197.2663 - val_loss: 213.2189\nEpoch 83/100\n23/23 - 0s - loss: 195.6654 - val_loss: 211.7303\nEpoch 84/100\n23/23 - 0s - loss: 194.1935 - val_loss: 209.9184\nEpoch 85/100\n23/23 - 0s - loss: 192.6330 - val_loss: 208.5513\nEpoch 86/100\n23/23 - 0s - loss: 191.2637 - val_loss: 206.6841\nEpoch 87/100\n23/23 - 0s - loss: 189.7782 - val_loss: 205.1495\nEpoch 88/100\n23/23 - 0s - loss: 188.4636 - val_loss: 204.2425\nEpoch 89/100\n23/23 - 0s - loss: 187.0043 - val_loss: 202.1676\nEpoch 90/100\n23/23 - 0s - loss: 185.5905 - val_loss: 200.5388\nEpoch 91/100\n23/23 - 0s - loss: 184.2005 - val_loss: 198.5555\nEpoch 92/100\n23/23 - 0s - loss: 182.8448 - val_loss: 197.4845\nEpoch 93/100\n23/23 - 0s - loss: 181.6733 - val_loss: 195.5082\nEpoch 94/100\n23/23 - 0s - loss: 180.1385 - val_loss: 194.4904\nEpoch 95/100\n23/23 - 0s - loss: 178.7840 - val_loss: 192.6168\nEpoch 96/100\n23/23 - 0s - loss: 177.4515 - val_loss: 191.4212\nEpoch 97/100\n23/23 - 0s - loss: 176.0040 - val_loss: 190.0905\nEpoch 98/100\n23/23 - 0s - loss: 174.6585 - val_loss: 188.1178\nEpoch 99/100\n23/23 - 0s - loss: 173.3415 - val_loss: 187.3602\nEpoch 100/100\n23/23 - 0s - loss: 171.8506 - val_loss: 185.1325\nEpoch 1/100\n23/23 - 0s - loss: 1663.0424 - val_loss: 1186.2963\nEpoch 2/100\n23/23 - 0s - loss: 1645.9974 - val_loss: 1172.5857\nEpoch 3/100\n23/23 - 0s - loss: 1629.3351 - val_loss: 1159.0056\nEpoch 4/100\n23/23 - 0s - loss: 1612.6779 - val_loss: 1145.7662\nEpoch 5/100\n23/23 - 0s - loss: 1595.5186 - val_loss: 1132.7290\nEpoch 6/100\n23/23 - 0s - loss: 1577.6707 - val_loss: 1119.2976\nEpoch 7/100\n23/23 - 0s - loss: 1559.0026 - val_loss: 1105.6622\nEpoch 8/100\n23/23 - 0s - loss: 1539.3324 - val_loss: 1091.6516\nEpoch 9/100\n23/23 - 0s - loss: 1518.5813 - val_loss: 1077.5590\nEpoch 10/100\n23/23 - 0s - loss: 1496.8547 - val_loss: 1063.4773\nEpoch 11/100\n23/23 - 0s - loss: 1474.2196 - val_loss: 1048.2859\nEpoch 12/100\n23/23 - 0s - loss: 1450.3163 - val_loss: 1032.9470\nEpoch 13/100\n23/23 - 0s - loss: 1425.0294 - val_loss: 1017.6181\nEpoch 14/100\n23/23 - 0s - loss: 1399.0471 - val_loss: 1001.6960\nEpoch 15/100\n23/23 - 0s - loss: 1371.9376 - val_loss: 985.7521\nEpoch 16/100\n23/23 - 0s - loss: 1343.9980 - val_loss: 969.0825\nEpoch 17/100\n23/23 - 0s - loss: 1314.1500 - val_loss: 952.2296\nEpoch 18/100\n23/23 - 0s - loss: 1284.0713 - val_loss: 934.9209\nEpoch 19/100\n23/23 - 0s - loss: 1252.9031 - val_loss: 917.3868\nEpoch 20/100\n23/23 - 0s - loss: 1221.3093 - val_loss: 899.7271\nEpoch 21/100\n23/23 - 0s - loss: 1188.9733 - val_loss: 881.4354\nEpoch 22/100\n23/23 - 0s - loss: 1156.1829 - val_loss: 863.0909\nEpoch 23/100\n23/23 - 0s - loss: 1123.0342 - val_loss: 844.1793\nEpoch 24/100\n23/23 - 0s - loss: 1089.0912 - val_loss: 825.2795\nEpoch 25/100\n23/23 - 0s - loss: 1055.3931 - val_loss: 805.8586\nEpoch 26/100\n23/23 - 0s - loss: 1021.0237 - val_loss: 787.1208\nEpoch 27/100\n23/23 - 0s - loss: 986.7443 - val_loss: 768.2015\nEpoch 28/100\n23/23 - 0s - loss: 952.3578 - val_loss: 748.4000\nEpoch 29/100\n23/23 - 0s - loss: 917.1232 - val_loss: 729.1954\nEpoch 30/100\n23/23 - 0s - loss: 882.2746 - val_loss: 709.7071\nEpoch 31/100\n23/23 - 0s - loss: 847.0349 - val_loss: 689.7538\nEpoch 32/100\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "23/23 - 0s - loss: 811.8577 - val_loss: 670.0251\nEpoch 33/100\n23/23 - 0s - loss: 777.2335 - val_loss: 650.0766\nEpoch 34/100\n23/23 - 0s - loss: 742.6596 - val_loss: 630.3504\nEpoch 35/100\n23/23 - 0s - loss: 709.2773 - val_loss: 610.7482\nEpoch 36/100\n23/23 - 0s - loss: 676.3238 - val_loss: 591.3393\nEpoch 37/100\n23/23 - 0s - loss: 644.9983 - val_loss: 571.8842\nEpoch 38/100\n23/23 - 0s - loss: 613.7666 - val_loss: 552.6703\nEpoch 39/100\n23/23 - 0s - loss: 584.1652 - val_loss: 534.3016\nEpoch 40/100\n23/23 - 0s - loss: 555.8149 - val_loss: 515.9534\nEpoch 41/100\n23/23 - 0s - loss: 528.7519 - val_loss: 497.9706\nEpoch 42/100\n23/23 - 0s - loss: 503.1714 - val_loss: 480.3805\nEpoch 43/100\n23/23 - 0s - loss: 478.5361 - val_loss: 464.2546\nEpoch 44/100\n23/23 - 0s - loss: 456.0822 - val_loss: 447.0589\nEpoch 45/100\n23/23 - 0s - loss: 434.1750 - val_loss: 431.2162\nEpoch 46/100\n23/23 - 0s - loss: 414.0988 - val_loss: 415.9479\nEpoch 47/100\n23/23 - 0s - loss: 395.2899 - val_loss: 400.9330\nEpoch 48/100\n23/23 - 0s - loss: 378.4733 - val_loss: 386.3828\nEpoch 49/100\n23/23 - 0s - loss: 361.8789 - val_loss: 373.0892\nEpoch 50/100\n23/23 - 0s - loss: 347.4244 - val_loss: 360.1297\nEpoch 51/100\n23/23 - 0s - loss: 333.8858 - val_loss: 348.0730\nEpoch 52/100\n23/23 - 0s - loss: 321.3374 - val_loss: 336.3273\nEpoch 53/100\n23/23 - 0s - loss: 310.3213 - val_loss: 324.9980\nEpoch 54/100\n23/23 - 0s - loss: 299.9688 - val_loss: 314.5663\nEpoch 55/100\n23/23 - 0s - loss: 290.5776 - val_loss: 305.2226\nEpoch 56/100\n23/23 - 0s - loss: 282.0113 - val_loss: 296.3232\nEpoch 57/100\n23/23 - 0s - loss: 274.4384 - val_loss: 287.9287\nEpoch 58/100\n23/23 - 0s - loss: 267.2661 - val_loss: 280.5300\nEpoch 59/100\n23/23 - 0s - loss: 260.9783 - val_loss: 272.6713\nEpoch 60/100\n23/23 - 0s - loss: 254.8986 - val_loss: 266.0457\nEpoch 61/100\n23/23 - 0s - loss: 249.5900 - val_loss: 259.8892\nEpoch 62/100\n23/23 - 0s - loss: 244.8913 - val_loss: 253.8781\nEpoch 63/100\n23/23 - 0s - loss: 240.3553 - val_loss: 248.8014\nEpoch 64/100\n23/23 - 0s - loss: 236.5233 - val_loss: 243.3455\nEpoch 65/100\n23/23 - 0s - loss: 232.6243 - val_loss: 239.4640\nEpoch 66/100\n23/23 - 0s - loss: 229.3161 - val_loss: 235.3146\nEpoch 67/100\n23/23 - 0s - loss: 225.9111 - val_loss: 231.3067\nEpoch 68/100\n23/23 - 0s - loss: 222.9761 - val_loss: 227.7199\nEpoch 69/100\n23/23 - 0s - loss: 220.1480 - val_loss: 224.4082\nEpoch 70/100\n23/23 - 0s - loss: 217.3643 - val_loss: 220.9144\nEpoch 71/100\n23/23 - 0s - loss: 214.8693 - val_loss: 218.0153\nEpoch 72/100\n23/23 - 0s - loss: 212.4679 - val_loss: 214.9849\nEpoch 73/100\n23/23 - 0s - loss: 210.1783 - val_loss: 211.9342\nEpoch 74/100\n23/23 - 0s - loss: 207.9735 - val_loss: 209.5800\nEpoch 75/100\n23/23 - 0s - loss: 205.9778 - val_loss: 206.9901\nEpoch 76/100\n23/23 - 0s - loss: 204.0095 - val_loss: 204.5974\nEpoch 77/100\n23/23 - 0s - loss: 202.1392 - val_loss: 202.7174\nEpoch 78/100\n23/23 - 0s - loss: 200.2787 - val_loss: 200.3523\nEpoch 79/100\n23/23 - 0s - loss: 198.5073 - val_loss: 197.5659\nEpoch 80/100\n23/23 - 0s - loss: 196.7817 - val_loss: 195.4838\nEpoch 81/100\n23/23 - 0s - loss: 195.1042 - val_loss: 193.5922\nEpoch 82/100\n23/23 - 0s - loss: 193.3961 - val_loss: 191.4709\nEpoch 83/100\n23/23 - 0s - loss: 191.9502 - val_loss: 189.4631\nEpoch 84/100\n23/23 - 0s - loss: 190.2580 - val_loss: 187.5187\nEpoch 85/100\n23/23 - 0s - loss: 188.8052 - val_loss: 185.6620\nEpoch 86/100\n23/23 - 0s - loss: 187.1934 - val_loss: 183.4620\nEpoch 87/100\n23/23 - 0s - loss: 185.6957 - val_loss: 182.0951\nEpoch 88/100\n23/23 - 0s - loss: 184.1426 - val_loss: 180.1732\nEpoch 89/100\n23/23 - 0s - loss: 182.7168 - val_loss: 178.5500\nEpoch 90/100\n23/23 - 0s - loss: 181.2148 - val_loss: 176.6070\nEpoch 91/100\n23/23 - 0s - loss: 179.7497 - val_loss: 175.1191\nEpoch 92/100\n23/23 - 0s - loss: 178.4421 - val_loss: 172.8827\nEpoch 93/100\n23/23 - 0s - loss: 176.8664 - val_loss: 171.5734\nEpoch 94/100\n23/23 - 0s - loss: 175.5359 - val_loss: 169.6728\nEpoch 95/100\n23/23 - 0s - loss: 174.0973 - val_loss: 167.7143\nEpoch 96/100\n23/23 - 0s - loss: 172.8272 - val_loss: 166.8669\nEpoch 97/100\n23/23 - 0s - loss: 171.4492 - val_loss: 165.0311\nEpoch 98/100\n23/23 - 0s - loss: 170.0558 - val_loss: 163.5903\nEpoch 99/100\n23/23 - 0s - loss: 168.8099 - val_loss: 162.5179\nEpoch 100/100\n23/23 - 0s - loss: 167.4269 - val_loss: 160.6651\nEpoch 1/100\n23/23 - 0s - loss: 1701.2966 - val_loss: 1250.6143\nEpoch 2/100\n23/23 - 0s - loss: 1683.1437 - val_loss: 1237.8011\nEpoch 3/100\n23/23 - 0s - loss: 1665.1979 - val_loss: 1224.7733\nEpoch 4/100\n23/23 - 0s - loss: 1647.0151 - val_loss: 1211.9351\nEpoch 5/100\n23/23 - 0s - loss: 1628.8339 - val_loss: 1198.2698\nEpoch 6/100\n23/23 - 0s - loss: 1609.7471 - val_loss: 1184.6637\nEpoch 7/100\n23/23 - 0s - loss: 1589.8680 - val_loss: 1171.0042\nEpoch 8/100\n23/23 - 0s - loss: 1569.3960 - val_loss: 1156.9309\nEpoch 9/100\n23/23 - 0s - loss: 1547.9636 - val_loss: 1142.5173\nEpoch 10/100\n23/23 - 0s - loss: 1525.5912 - val_loss: 1127.9252\nEpoch 11/100\n23/23 - 0s - loss: 1502.2627 - val_loss: 1112.7726\nEpoch 12/100\n23/23 - 0s - loss: 1477.8242 - val_loss: 1097.4496\nEpoch 13/100\n23/23 - 0s - loss: 1452.3177 - val_loss: 1081.4233\nEpoch 14/100\n23/23 - 0s - loss: 1425.8523 - val_loss: 1065.0900\nEpoch 15/100\n23/23 - 0s - loss: 1397.9200 - val_loss: 1048.5553\nEpoch 16/100\n23/23 - 0s - loss: 1368.9694 - val_loss: 1030.5240\nEpoch 17/100\n23/23 - 0s - loss: 1338.5513 - val_loss: 1012.7747\nEpoch 18/100\n23/23 - 0s - loss: 1306.5148 - val_loss: 994.1813\nEpoch 19/100\n23/23 - 0s - loss: 1274.0464 - val_loss: 975.7771\nEpoch 20/100\n23/23 - 0s - loss: 1240.3547 - val_loss: 956.6412\nEpoch 21/100\n23/23 - 0s - loss: 1205.2290 - val_loss: 937.7888\nEpoch 22/100\n23/23 - 0s - loss: 1170.3818 - val_loss: 917.6514\nEpoch 23/100\n23/23 - 0s - loss: 1134.4041 - val_loss: 897.5782\nEpoch 24/100\n23/23 - 0s - loss: 1097.8970 - val_loss: 877.5591\nEpoch 25/100\n23/23 - 0s - loss: 1061.1270 - val_loss: 857.8176\nEpoch 26/100\n23/23 - 0s - loss: 1024.7760 - val_loss: 837.3929\nEpoch 27/100\n23/23 - 0s - loss: 988.0347 - val_loss: 817.5187\nEpoch 28/100\n23/23 - 0s - loss: 951.4636 - val_loss: 796.9137\nEpoch 29/100\n23/23 - 0s - loss: 915.6478 - val_loss: 777.1059\nEpoch 30/100\n23/23 - 0s - loss: 880.0809 - val_loss: 756.7220\nEpoch 31/100\n23/23 - 0s - loss: 845.7632 - val_loss: 736.0604\nEpoch 32/100\n23/23 - 0s - loss: 811.0336 - val_loss: 715.9464\nEpoch 33/100\n23/23 - 0s - loss: 777.4437 - val_loss: 696.0774\nEpoch 34/100\n23/23 - 0s - loss: 744.4670 - val_loss: 675.9406\nEpoch 35/100\n23/23 - 0s - loss: 711.9189 - val_loss: 656.9559\nEpoch 36/100\n23/23 - 0s - loss: 680.3994 - val_loss: 637.4320\nEpoch 37/100\n23/23 - 0s - loss: 650.2814 - val_loss: 617.6761\nEpoch 38/100\n23/23 - 0s - loss: 620.5552 - val_loss: 598.6207\nEpoch 39/100\n23/23 - 0s - loss: 592.5786 - val_loss: 579.9174\nEpoch 40/100\n23/23 - 0s - loss: 565.6619 - val_loss: 561.1526\nEpoch 41/100\n23/23 - 0s - loss: 539.4216 - val_loss: 543.1490\nEpoch 42/100\n23/23 - 0s - loss: 515.6171 - val_loss: 525.0649\nEpoch 43/100\n23/23 - 0s - loss: 492.5167 - val_loss: 508.0512\nEpoch 44/100\n23/23 - 0s - loss: 471.0847 - val_loss: 491.2946\nEpoch 45/100\n23/23 - 0s - loss: 450.5746 - val_loss: 475.8701\nEpoch 46/100\n23/23 - 0s - loss: 432.1345 - val_loss: 460.0009\nEpoch 47/100\n23/23 - 0s - loss: 414.1681 - val_loss: 445.5045\nEpoch 48/100\n23/23 - 0s - loss: 397.9675 - val_loss: 430.8699\nEpoch 49/100\n23/23 - 0s - loss: 382.5596 - val_loss: 417.2763\nEpoch 50/100\n23/23 - 0s - loss: 369.0970 - val_loss: 403.5711\nEpoch 51/100\n23/23 - 0s - loss: 355.8413 - val_loss: 392.3125\nEpoch 52/100\n23/23 - 0s - loss: 343.9941 - val_loss: 380.2538\nEpoch 53/100\n23/23 - 0s - loss: 333.1781 - val_loss: 368.7787\nEpoch 54/100\n23/23 - 0s - loss: 322.9138 - val_loss: 358.0764\nEpoch 55/100\n23/23 - 0s - loss: 313.6284 - val_loss: 348.0247\nEpoch 56/100\n23/23 - 0s - loss: 304.9603 - val_loss: 338.9415\nEpoch 57/100\n23/23 - 0s - loss: 297.2759 - val_loss: 329.1478\nEpoch 58/100\n23/23 - 0s - loss: 289.7023 - val_loss: 320.8537\nEpoch 59/100\n23/23 - 0s - loss: 283.1152 - val_loss: 312.3314\nEpoch 60/100\n23/23 - 0s - loss: 276.3981 - val_loss: 304.3982\nEpoch 61/100\n23/23 - 0s - loss: 270.6225 - val_loss: 296.7054\nEpoch 62/100\n23/23 - 0s - loss: 265.0770 - val_loss: 290.3445\nEpoch 63/100\n23/23 - 0s - loss: 259.9771 - val_loss: 283.5236\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 64/100\n23/23 - 0s - loss: 255.2610 - val_loss: 276.6278\nEpoch 65/100\n23/23 - 0s - loss: 250.6940 - val_loss: 270.2590\nEpoch 66/100\n23/23 - 0s - loss: 246.5369 - val_loss: 264.3174\nEpoch 67/100\n23/23 - 0s - loss: 242.4501 - val_loss: 258.3943\nEpoch 68/100\n23/23 - 0s - loss: 238.7345 - val_loss: 253.6198\nEpoch 69/100\n23/23 - 0s - loss: 235.2424 - val_loss: 248.0445\nEpoch 70/100\n23/23 - 0s - loss: 231.7173 - val_loss: 243.6356\nEpoch 71/100\n23/23 - 0s - loss: 228.5573 - val_loss: 239.1288\nEpoch 72/100\n23/23 - 0s - loss: 225.4667 - val_loss: 234.9981\nEpoch 73/100\n23/23 - 0s - loss: 222.4892 - val_loss: 230.7156\nEpoch 74/100\n23/23 - 0s - loss: 219.5216 - val_loss: 226.8733\nEpoch 75/100\n23/23 - 0s - loss: 216.8838 - val_loss: 223.2368\nEpoch 76/100\n23/23 - 0s - loss: 214.3104 - val_loss: 219.0152\nEpoch 77/100\n23/23 - 0s - loss: 211.6296 - val_loss: 216.3257\nEpoch 78/100\n23/23 - 0s - loss: 209.3219 - val_loss: 212.8776\nEpoch 79/100\n23/23 - 0s - loss: 206.8849 - val_loss: 210.3243\nEpoch 80/100\n23/23 - 0s - loss: 204.5904 - val_loss: 207.1804\nEpoch 81/100\n23/23 - 0s - loss: 202.4694 - val_loss: 205.0802\nEpoch 82/100\n23/23 - 0s - loss: 200.3146 - val_loss: 201.9602\nEpoch 83/100\n23/23 - 0s - loss: 198.0956 - val_loss: 199.9608\nEpoch 84/100\n23/23 - 0s - loss: 196.1476 - val_loss: 197.7678\nEpoch 85/100\n23/23 - 0s - loss: 193.9464 - val_loss: 195.2566\nEpoch 86/100\n23/23 - 0s - loss: 192.0410 - val_loss: 193.0282\nEpoch 87/100\n23/23 - 0s - loss: 189.9996 - val_loss: 191.0608\nEpoch 88/100\n23/23 - 0s - loss: 188.0674 - val_loss: 189.2733\nEpoch 89/100\n23/23 - 0s - loss: 186.2531 - val_loss: 186.9350\nEpoch 90/100\n23/23 - 0s - loss: 184.2493 - val_loss: 185.1379\nEpoch 91/100\n23/23 - 0s - loss: 182.4352 - val_loss: 183.5730\nEpoch 92/100\n23/23 - 0s - loss: 180.6276 - val_loss: 182.4917\nEpoch 93/100\n23/23 - 0s - loss: 178.8281 - val_loss: 180.9520\nEpoch 94/100\n23/23 - 0s - loss: 177.0928 - val_loss: 179.3854\nEpoch 95/100\n23/23 - 0s - loss: 175.4435 - val_loss: 178.1357\nEpoch 96/100\n23/23 - 0s - loss: 173.7470 - val_loss: 176.6006\nEpoch 97/100\n23/23 - 0s - loss: 172.0835 - val_loss: 175.6518\nEpoch 98/100\n23/23 - 0s - loss: 170.5747 - val_loss: 174.4119\nEpoch 99/100\n23/23 - 0s - loss: 169.0488 - val_loss: 173.1917\nEpoch 100/100\n23/23 - 0s - loss: 167.4885 - val_loss: 172.1835\n"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Report the mean and the standard deviation of the mean squared errors."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print(df_norm.shape)\ndf_norm",
            "execution_count": 21,
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "(100, 5)\n"
                },
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>repeat_1</th>\n      <th>repeat_2</th>\n      <th>repeat_3</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1667.797852</td>\n      <td>1663.042358</td>\n      <td>1701.296631</td>\n      <td>1677.378947</td>\n      <td>20.849350</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1647.648560</td>\n      <td>1645.997437</td>\n      <td>1683.143677</td>\n      <td>1658.929891</td>\n      <td>20.985998</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1627.257324</td>\n      <td>1629.335083</td>\n      <td>1665.197876</td>\n      <td>1640.596761</td>\n      <td>21.330504</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1606.331787</td>\n      <td>1612.677856</td>\n      <td>1647.015137</td>\n      <td>1622.008260</td>\n      <td>21.887806</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1584.589233</td>\n      <td>1595.518555</td>\n      <td>1628.833862</td>\n      <td>1602.980550</td>\n      <td>23.046862</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>177.451462</td>\n      <td>172.827240</td>\n      <td>173.747025</td>\n      <td>174.675242</td>\n      <td>2.447866</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>176.003952</td>\n      <td>171.449219</td>\n      <td>172.083481</td>\n      <td>173.178884</td>\n      <td>2.467049</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>174.658524</td>\n      <td>170.055756</td>\n      <td>170.574738</td>\n      <td>171.763006</td>\n      <td>2.520983</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>173.341476</td>\n      <td>168.809921</td>\n      <td>169.048752</td>\n      <td>170.400050</td>\n      <td>2.550148</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>171.850616</td>\n      <td>167.426895</td>\n      <td>167.488464</td>\n      <td>168.921992</td>\n      <td>2.536450</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows \u00d7 5 columns</p>\n</div>",
                        "text/plain": "       repeat_1     repeat_2     repeat_3         mean        std\n0   1667.797852  1663.042358  1701.296631  1677.378947  20.849350\n1   1647.648560  1645.997437  1683.143677  1658.929891  20.985998\n2   1627.257324  1629.335083  1665.197876  1640.596761  21.330504\n3   1606.331787  1612.677856  1647.015137  1622.008260  21.887806\n4   1584.589233  1595.518555  1628.833862  1602.980550  23.046862\n..          ...          ...          ...          ...        ...\n95   177.451462   172.827240   173.747025   174.675242   2.447866\n96   176.003952   171.449219   172.083481   173.178884   2.467049\n97   174.658524   170.055756   170.574738   171.763006   2.520983\n98   173.341476   168.809921   169.048752   170.400050   2.550148\n99   171.850616   167.426895   167.488464   168.921992   2.536450\n\n[100 rows x 5 columns]"
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### How does the mean of the mean squared errors compare to that from Step B?"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Using 100 epochs, better results are obtained, achieving better values than in step A and step B, although the values are close to step A, in step C a much smaller standard deviation is obtained."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"item36\"></a>"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## D. Increase the number of hidden layers "
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "# define regression model\ndef regression_model_B():\n    # create model\n    model = Sequential()\n    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(1))\n    \n    # compile model\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model",
            "execution_count": 22,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# fit the model\ndf_norm = pd.DataFrame()\nrepeats = 3\nfor repeat in range(repeats):\n    model = regression_model_B()\n    history_callback = model.fit(predictors_norm, target, validation_split=0.3, epochs=50, verbose=2)\n    \n    layer_name = 'repeat_{}'.format(repeat + 1) # otherwise give the layer a number\n    df_norm[layer_name] = history_callback.history[\"loss\"]\n    \n    if repeat == repeats-1 :\n        df_norm['mean'] = df_norm.mean(axis=1)\n        df_norm['std'] = df_norm.iloc[:,0:repeats].std(axis=1)",
            "execution_count": 23,
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/50\n23/23 - 0s - loss: 1707.1511 - val_loss: 1226.1625\nEpoch 2/50\n23/23 - 0s - loss: 1684.9954 - val_loss: 1206.8888\nEpoch 3/50\n23/23 - 0s - loss: 1657.2130 - val_loss: 1181.4323\nEpoch 4/50\n23/23 - 0s - loss: 1617.2899 - val_loss: 1145.4061\nEpoch 5/50\n23/23 - 0s - loss: 1561.1637 - val_loss: 1096.4148\nEpoch 6/50\n23/23 - 0s - loss: 1484.7805 - val_loss: 1031.6111\nEpoch 7/50\n23/23 - 0s - loss: 1381.6160 - val_loss: 946.0930\nEpoch 8/50\n23/23 - 0s - loss: 1243.9788 - val_loss: 838.0406\nEpoch 9/50\n23/23 - 0s - loss: 1065.6915 - val_loss: 703.3225\nEpoch 10/50\n23/23 - 0s - loss: 849.5839 - val_loss: 546.9659\nEpoch 11/50\n23/23 - 0s - loss: 624.0748 - val_loss: 398.6194\nEpoch 12/50\n23/23 - 0s - loss: 430.2958 - val_loss: 290.9235\nEpoch 13/50\n23/23 - 0s - loss: 308.4342 - val_loss: 232.2239\nEpoch 14/50\n23/23 - 0s - loss: 249.0766 - val_loss: 211.6922\nEpoch 15/50\n23/23 - 0s - loss: 224.3861 - val_loss: 200.8786\nEpoch 16/50\n23/23 - 0s - loss: 211.7305 - val_loss: 195.9780\nEpoch 17/50\n23/23 - 0s - loss: 203.6808 - val_loss: 190.9046\nEpoch 18/50\n23/23 - 0s - loss: 198.1601 - val_loss: 187.4230\nEpoch 19/50\n23/23 - 0s - loss: 192.9990 - val_loss: 182.5867\nEpoch 20/50\n23/23 - 0s - loss: 188.9641 - val_loss: 180.1238\nEpoch 21/50\n23/23 - 0s - loss: 185.4480 - val_loss: 177.2731\nEpoch 22/50\n23/23 - 0s - loss: 182.4611 - val_loss: 176.9759\nEpoch 23/50\n23/23 - 0s - loss: 179.5478 - val_loss: 172.1064\nEpoch 24/50\n23/23 - 0s - loss: 176.9941 - val_loss: 172.0764\nEpoch 25/50\n23/23 - 0s - loss: 174.7025 - val_loss: 170.1965\nEpoch 26/50\n23/23 - 0s - loss: 172.7652 - val_loss: 167.1555\nEpoch 27/50\n23/23 - 0s - loss: 170.9672 - val_loss: 165.4179\nEpoch 28/50\n23/23 - 0s - loss: 168.9705 - val_loss: 164.0847\nEpoch 29/50\n23/23 - 0s - loss: 167.2182 - val_loss: 162.9696\nEpoch 30/50\n23/23 - 0s - loss: 165.4837 - val_loss: 160.9820\nEpoch 31/50\n23/23 - 0s - loss: 164.4439 - val_loss: 158.7210\nEpoch 32/50\n23/23 - 0s - loss: 162.7351 - val_loss: 156.8915\nEpoch 33/50\n23/23 - 0s - loss: 161.0379 - val_loss: 156.3118\nEpoch 34/50\n23/23 - 0s - loss: 159.9311 - val_loss: 154.0765\nEpoch 35/50\n23/23 - 0s - loss: 158.4415 - val_loss: 153.3623\nEpoch 36/50\n23/23 - 0s - loss: 157.0619 - val_loss: 151.4381\nEpoch 37/50\n23/23 - 0s - loss: 155.7814 - val_loss: 150.6507\nEpoch 38/50\n23/23 - 0s - loss: 154.5789 - val_loss: 148.2653\nEpoch 39/50\n23/23 - 0s - loss: 153.1787 - val_loss: 148.0697\nEpoch 40/50\n23/23 - 0s - loss: 152.2340 - val_loss: 146.5947\nEpoch 41/50\n23/23 - 0s - loss: 150.9442 - val_loss: 146.0444\nEpoch 42/50\n23/23 - 0s - loss: 150.0700 - val_loss: 144.9779\nEpoch 43/50\n23/23 - 0s - loss: 149.1871 - val_loss: 142.7764\nEpoch 44/50\n23/23 - 0s - loss: 147.8462 - val_loss: 143.3483\nEpoch 45/50\n23/23 - 0s - loss: 147.2022 - val_loss: 142.6792\nEpoch 46/50\n23/23 - 0s - loss: 146.1757 - val_loss: 143.2300\nEpoch 47/50\n23/23 - 0s - loss: 145.3148 - val_loss: 141.6279\nEpoch 48/50\n23/23 - 0s - loss: 144.7979 - val_loss: 141.8684\nEpoch 49/50\n23/23 - 0s - loss: 143.7754 - val_loss: 140.6298\nEpoch 50/50\n23/23 - 0s - loss: 142.8945 - val_loss: 140.6787\nEpoch 1/50\n23/23 - 0s - loss: 1688.0645 - val_loss: 1218.9371\nEpoch 2/50\n23/23 - 0s - loss: 1663.5549 - val_loss: 1198.0310\nEpoch 3/50\n23/23 - 0s - loss: 1624.6326 - val_loss: 1162.2026\nEpoch 4/50\n23/23 - 0s - loss: 1562.8423 - val_loss: 1101.1158\nEpoch 5/50\n23/23 - 0s - loss: 1463.1626 - val_loss: 1004.6614\nEpoch 6/50\n23/23 - 0s - loss: 1316.1924 - val_loss: 869.7637\nEpoch 7/50\n23/23 - 0s - loss: 1116.5524 - val_loss: 698.3314\nEpoch 8/50\n23/23 - 0s - loss: 878.5625 - val_loss: 510.1630\nEpoch 9/50\n23/23 - 0s - loss: 641.8848 - val_loss: 343.2674\nEpoch 10/50\n23/23 - 0s - loss: 460.5827 - val_loss: 239.1935\nEpoch 11/50\n23/23 - 0s - loss: 349.8748 - val_loss: 200.8234\nEpoch 12/50\n23/23 - 0s - loss: 298.7581 - val_loss: 185.5562\nEpoch 13/50\n23/23 - 0s - loss: 268.2153 - val_loss: 179.8239\nEpoch 14/50\n23/23 - 0s - loss: 249.8338 - val_loss: 174.2745\nEpoch 15/50\n23/23 - 0s - loss: 235.3917 - val_loss: 171.2492\nEpoch 16/50\n23/23 - 0s - loss: 224.4030 - val_loss: 166.7630\nEpoch 17/50\n23/23 - 0s - loss: 214.4838 - val_loss: 164.2037\nEpoch 18/50\n23/23 - 0s - loss: 206.4569 - val_loss: 161.8857\nEpoch 19/50\n23/23 - 0s - loss: 199.5857 - val_loss: 161.6622\nEpoch 20/50\n23/23 - 0s - loss: 193.6780 - val_loss: 160.5836\nEpoch 21/50\n23/23 - 0s - loss: 188.6855 - val_loss: 160.2394\nEpoch 22/50\n23/23 - 0s - loss: 183.9312 - val_loss: 159.2213\nEpoch 23/50\n23/23 - 0s - loss: 180.3674 - val_loss: 160.1652\nEpoch 24/50\n23/23 - 0s - loss: 177.0683 - val_loss: 160.6407\nEpoch 25/50\n23/23 - 0s - loss: 174.6441 - val_loss: 160.0501\nEpoch 26/50\n23/23 - 0s - loss: 171.4015 - val_loss: 161.5850\nEpoch 27/50\n23/23 - 0s - loss: 169.5249 - val_loss: 162.5927\nEpoch 28/50\n23/23 - 0s - loss: 166.7613 - val_loss: 161.9248\nEpoch 29/50\n23/23 - 0s - loss: 165.1692 - val_loss: 161.8499\nEpoch 30/50\n23/23 - 0s - loss: 162.9675 - val_loss: 163.2244\nEpoch 31/50\n23/23 - 0s - loss: 161.3018 - val_loss: 164.9331\nEpoch 32/50\n23/23 - 0s - loss: 159.4756 - val_loss: 165.5215\nEpoch 33/50\n23/23 - 0s - loss: 158.2305 - val_loss: 165.1503\nEpoch 34/50\n23/23 - 0s - loss: 156.8122 - val_loss: 168.3276\nEpoch 35/50\n23/23 - 0s - loss: 155.2489 - val_loss: 165.9388\nEpoch 36/50\n23/23 - 0s - loss: 154.2204 - val_loss: 167.0606\nEpoch 37/50\n23/23 - 0s - loss: 152.6763 - val_loss: 168.5658\nEpoch 38/50\n23/23 - 0s - loss: 152.2199 - val_loss: 169.5493\nEpoch 39/50\n23/23 - 0s - loss: 150.4772 - val_loss: 167.1510\nEpoch 40/50\n23/23 - 0s - loss: 149.7188 - val_loss: 167.3667\nEpoch 41/50\n23/23 - 0s - loss: 149.0583 - val_loss: 171.0952\nEpoch 42/50\n23/23 - 0s - loss: 148.1329 - val_loss: 167.6371\nEpoch 43/50\n23/23 - 0s - loss: 147.2226 - val_loss: 169.5624\nEpoch 44/50\n23/23 - 0s - loss: 146.2970 - val_loss: 170.0036\nEpoch 45/50\n23/23 - 0s - loss: 145.5098 - val_loss: 170.1489\nEpoch 46/50\n23/23 - 0s - loss: 144.7406 - val_loss: 170.7759\nEpoch 47/50\n23/23 - 0s - loss: 144.1733 - val_loss: 168.9248\nEpoch 48/50\n23/23 - 0s - loss: 143.6586 - val_loss: 170.5208\nEpoch 49/50\n23/23 - 0s - loss: 142.8032 - val_loss: 168.8320\nEpoch 50/50\n23/23 - 0s - loss: 142.4016 - val_loss: 167.7491\nEpoch 1/50\n23/23 - 0s - loss: 1697.4741 - val_loss: 1223.2771\nEpoch 2/50\n23/23 - 0s - loss: 1677.4176 - val_loss: 1207.7485\nEpoch 3/50\n23/23 - 0s - loss: 1654.4946 - val_loss: 1188.5098\nEpoch 4/50\n23/23 - 0s - loss: 1625.2520 - val_loss: 1162.9055\nEpoch 5/50\n23/23 - 0s - loss: 1585.2133 - val_loss: 1127.7035\nEpoch 6/50\n23/23 - 0s - loss: 1528.5946 - val_loss: 1077.9249\nEpoch 7/50\n23/23 - 0s - loss: 1448.0776 - val_loss: 1004.5466\nEpoch 8/50\n23/23 - 0s - loss: 1327.5435 - val_loss: 898.0814\nEpoch 9/50\n23/23 - 0s - loss: 1151.1862 - val_loss: 755.3213\nEpoch 10/50\n23/23 - 0s - loss: 921.0823 - val_loss: 588.2653\nEpoch 11/50\n23/23 - 0s - loss: 682.8219 - val_loss: 432.5079\nEpoch 12/50\n23/23 - 0s - loss: 488.7231 - val_loss: 312.4481\nEpoch 13/50\n23/23 - 0s - loss: 361.1210 - val_loss: 245.2054\nEpoch 14/50\n23/23 - 0s - loss: 296.7132 - val_loss: 216.1076\nEpoch 15/50\n23/23 - 0s - loss: 265.0226 - val_loss: 204.7192\nEpoch 16/50\n23/23 - 0s - loss: 246.1445 - val_loss: 200.1346\nEpoch 17/50\n23/23 - 0s - loss: 232.6893 - val_loss: 194.1494\nEpoch 18/50\n23/23 - 0s - loss: 223.3764 - val_loss: 192.2820\nEpoch 19/50\n23/23 - 0s - loss: 214.8216 - val_loss: 183.4366\nEpoch 20/50\n23/23 - 0s - loss: 207.3949 - val_loss: 181.7049\nEpoch 21/50\n23/23 - 0s - loss: 201.6508 - val_loss: 176.9700\nEpoch 22/50\n23/23 - 0s - loss: 196.1109 - val_loss: 172.8741\nEpoch 23/50\n23/23 - 0s - loss: 191.5262 - val_loss: 169.4368\nEpoch 24/50\n23/23 - 0s - loss: 187.1076 - val_loss: 167.1767\nEpoch 25/50\n23/23 - 0s - loss: 183.5281 - val_loss: 164.6472\nEpoch 26/50\n23/23 - 0s - loss: 179.8562 - val_loss: 162.1623\nEpoch 27/50\n23/23 - 0s - loss: 176.6445 - val_loss: 160.0032\nEpoch 28/50\n23/23 - 0s - loss: 173.8348 - val_loss: 159.1346\nEpoch 29/50\n23/23 - 0s - loss: 171.6186 - val_loss: 158.4317\nEpoch 30/50\n23/23 - 0s - loss: 168.6682 - val_loss: 155.9836\nEpoch 31/50\n23/23 - 0s - loss: 166.1606 - val_loss: 153.5432\nEpoch 32/50\n23/23 - 0s - loss: 163.9742 - val_loss: 151.3628\nEpoch 33/50\n23/23 - 0s - loss: 161.1822 - val_loss: 150.8160\nEpoch 34/50\n23/23 - 0s - loss: 158.9707 - val_loss: 149.6798\nEpoch 35/50\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "23/23 - 0s - loss: 156.6962 - val_loss: 149.1887\nEpoch 36/50\n23/23 - 0s - loss: 154.5953 - val_loss: 147.8427\nEpoch 37/50\n23/23 - 0s - loss: 152.2889 - val_loss: 145.3190\nEpoch 38/50\n23/23 - 0s - loss: 149.7756 - val_loss: 143.2502\nEpoch 39/50\n23/23 - 0s - loss: 147.7473 - val_loss: 141.7266\nEpoch 40/50\n23/23 - 0s - loss: 145.2357 - val_loss: 141.6100\nEpoch 41/50\n23/23 - 0s - loss: 142.9976 - val_loss: 139.7146\nEpoch 42/50\n23/23 - 0s - loss: 140.7312 - val_loss: 138.2389\nEpoch 43/50\n23/23 - 0s - loss: 138.7255 - val_loss: 137.5203\nEpoch 44/50\n23/23 - 0s - loss: 136.3338 - val_loss: 134.4845\nEpoch 45/50\n23/23 - 0s - loss: 134.4210 - val_loss: 133.0623\nEpoch 46/50\n23/23 - 0s - loss: 132.4233 - val_loss: 132.8895\nEpoch 47/50\n23/23 - 0s - loss: 129.8069 - val_loss: 129.0739\nEpoch 48/50\n23/23 - 0s - loss: 127.6919 - val_loss: 129.7571\nEpoch 49/50\n23/23 - 0s - loss: 125.1745 - val_loss: 127.7702\nEpoch 50/50\n23/23 - 0s - loss: 123.0651 - val_loss: 126.2103\n"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Report the mean and the standard deviation of the mean squared errors."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "print(df_norm.shape)\ndf_norm",
            "execution_count": 24,
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "(50, 5)\n"
                },
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>repeat_1</th>\n      <th>repeat_2</th>\n      <th>repeat_3</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1707.151123</td>\n      <td>1688.064453</td>\n      <td>1697.474121</td>\n      <td>1697.563232</td>\n      <td>9.543647</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1684.995361</td>\n      <td>1663.554932</td>\n      <td>1677.417603</td>\n      <td>1675.322632</td>\n      <td>10.872658</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1657.213013</td>\n      <td>1624.632568</td>\n      <td>1654.494629</td>\n      <td>1645.446737</td>\n      <td>18.076770</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1617.289917</td>\n      <td>1562.842285</td>\n      <td>1625.251953</td>\n      <td>1601.794718</td>\n      <td>33.967890</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1561.163696</td>\n      <td>1463.162598</td>\n      <td>1585.213257</td>\n      <td>1536.513184</td>\n      <td>64.651579</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1484.780518</td>\n      <td>1316.192383</td>\n      <td>1528.594604</td>\n      <td>1443.189168</td>\n      <td>112.143017</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1381.615967</td>\n      <td>1116.552368</td>\n      <td>1448.077637</td>\n      <td>1315.415324</td>\n      <td>175.397106</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1243.978760</td>\n      <td>878.562500</td>\n      <td>1327.543457</td>\n      <td>1150.028239</td>\n      <td>238.780234</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1065.691528</td>\n      <td>641.884827</td>\n      <td>1151.186157</td>\n      <td>952.920837</td>\n      <td>272.735922</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>849.583923</td>\n      <td>460.582703</td>\n      <td>921.082336</td>\n      <td>743.749654</td>\n      <td>247.821806</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>624.074768</td>\n      <td>349.874847</td>\n      <td>682.821899</td>\n      <td>552.257172</td>\n      <td>177.712578</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>430.295776</td>\n      <td>298.758087</td>\n      <td>488.723114</td>\n      <td>405.925659</td>\n      <td>97.299048</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>308.434174</td>\n      <td>268.215332</td>\n      <td>361.121033</td>\n      <td>312.590179</td>\n      <td>46.592077</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>249.076569</td>\n      <td>249.833847</td>\n      <td>296.713165</td>\n      <td>265.207860</td>\n      <td>27.287022</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>224.386139</td>\n      <td>235.391663</td>\n      <td>265.022552</td>\n      <td>241.600118</td>\n      <td>21.017569</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>211.730499</td>\n      <td>224.402954</td>\n      <td>246.144455</td>\n      <td>227.425969</td>\n      <td>17.405001</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>203.680786</td>\n      <td>214.483826</td>\n      <td>232.689255</td>\n      <td>216.951289</td>\n      <td>14.660801</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>198.160065</td>\n      <td>206.456879</td>\n      <td>223.376404</td>\n      <td>209.331116</td>\n      <td>12.851532</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>192.999023</td>\n      <td>199.585693</td>\n      <td>214.821564</td>\n      <td>202.468760</td>\n      <td>11.193296</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>188.964066</td>\n      <td>193.678009</td>\n      <td>207.394913</td>\n      <td>196.678996</td>\n      <td>9.574888</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>185.447983</td>\n      <td>188.685455</td>\n      <td>201.650848</td>\n      <td>191.928095</td>\n      <td>8.574337</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>182.461060</td>\n      <td>183.931168</td>\n      <td>196.110855</td>\n      <td>187.501027</td>\n      <td>7.492473</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>179.547821</td>\n      <td>180.367401</td>\n      <td>191.526154</td>\n      <td>183.813792</td>\n      <td>6.691660</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>176.994110</td>\n      <td>177.068344</td>\n      <td>187.107605</td>\n      <td>180.390020</td>\n      <td>5.817718</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>174.702515</td>\n      <td>174.644058</td>\n      <td>183.528061</td>\n      <td>177.624878</td>\n      <td>5.112390</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>172.765198</td>\n      <td>171.401520</td>\n      <td>179.856155</td>\n      <td>174.674291</td>\n      <td>4.539129</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>170.967178</td>\n      <td>169.524918</td>\n      <td>176.644455</td>\n      <td>172.378850</td>\n      <td>3.763850</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>168.970490</td>\n      <td>166.761307</td>\n      <td>173.834793</td>\n      <td>169.855530</td>\n      <td>3.618843</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>167.218201</td>\n      <td>165.169189</td>\n      <td>171.618591</td>\n      <td>168.001994</td>\n      <td>3.295367</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>165.483673</td>\n      <td>162.967484</td>\n      <td>168.668228</td>\n      <td>165.706462</td>\n      <td>2.856895</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>164.443878</td>\n      <td>161.301773</td>\n      <td>166.160553</td>\n      <td>163.968735</td>\n      <td>2.463992</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>162.735077</td>\n      <td>159.475647</td>\n      <td>163.974182</td>\n      <td>162.061635</td>\n      <td>2.323650</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>161.037888</td>\n      <td>158.230453</td>\n      <td>161.182220</td>\n      <td>160.150187</td>\n      <td>1.664104</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>159.931137</td>\n      <td>156.812210</td>\n      <td>158.970718</td>\n      <td>158.571355</td>\n      <td>1.597356</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>158.441544</td>\n      <td>155.248871</td>\n      <td>156.696228</td>\n      <td>156.795547</td>\n      <td>1.598652</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>157.061874</td>\n      <td>154.220444</td>\n      <td>154.595306</td>\n      <td>155.292542</td>\n      <td>1.543708</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>155.781403</td>\n      <td>152.676346</td>\n      <td>152.288910</td>\n      <td>153.582219</td>\n      <td>1.914375</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>154.578934</td>\n      <td>152.219925</td>\n      <td>149.775589</td>\n      <td>152.191483</td>\n      <td>2.401799</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>153.178741</td>\n      <td>150.477234</td>\n      <td>147.747345</td>\n      <td>150.467773</td>\n      <td>2.715711</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>152.234009</td>\n      <td>149.718765</td>\n      <td>145.235687</td>\n      <td>149.062820</td>\n      <td>3.544972</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>150.944183</td>\n      <td>149.058334</td>\n      <td>142.997604</td>\n      <td>147.666707</td>\n      <td>4.152048</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>150.069992</td>\n      <td>148.132904</td>\n      <td>140.731216</td>\n      <td>146.311371</td>\n      <td>4.928658</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>149.187103</td>\n      <td>147.222626</td>\n      <td>138.725540</td>\n      <td>145.045090</td>\n      <td>5.560335</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>147.846161</td>\n      <td>146.297012</td>\n      <td>136.333755</td>\n      <td>143.492310</td>\n      <td>6.247691</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>147.202209</td>\n      <td>145.509750</td>\n      <td>134.421036</td>\n      <td>142.377665</td>\n      <td>6.942411</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>146.175735</td>\n      <td>144.740646</td>\n      <td>132.423309</td>\n      <td>141.113230</td>\n      <td>7.559822</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>145.314835</td>\n      <td>144.173340</td>\n      <td>129.806870</td>\n      <td>139.765015</td>\n      <td>8.642872</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>144.797913</td>\n      <td>143.658554</td>\n      <td>127.691917</td>\n      <td>138.716128</td>\n      <td>9.564228</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>143.775406</td>\n      <td>142.803192</td>\n      <td>125.174484</td>\n      <td>137.251027</td>\n      <td>10.469884</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>142.894455</td>\n      <td>142.401550</td>\n      <td>123.065132</td>\n      <td>136.120379</td>\n      <td>11.308861</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "       repeat_1     repeat_2     repeat_3         mean         std\n0   1707.151123  1688.064453  1697.474121  1697.563232    9.543647\n1   1684.995361  1663.554932  1677.417603  1675.322632   10.872658\n2   1657.213013  1624.632568  1654.494629  1645.446737   18.076770\n3   1617.289917  1562.842285  1625.251953  1601.794718   33.967890\n4   1561.163696  1463.162598  1585.213257  1536.513184   64.651579\n5   1484.780518  1316.192383  1528.594604  1443.189168  112.143017\n6   1381.615967  1116.552368  1448.077637  1315.415324  175.397106\n7   1243.978760   878.562500  1327.543457  1150.028239  238.780234\n8   1065.691528   641.884827  1151.186157   952.920837  272.735922\n9    849.583923   460.582703   921.082336   743.749654  247.821806\n10   624.074768   349.874847   682.821899   552.257172  177.712578\n11   430.295776   298.758087   488.723114   405.925659   97.299048\n12   308.434174   268.215332   361.121033   312.590179   46.592077\n13   249.076569   249.833847   296.713165   265.207860   27.287022\n14   224.386139   235.391663   265.022552   241.600118   21.017569\n15   211.730499   224.402954   246.144455   227.425969   17.405001\n16   203.680786   214.483826   232.689255   216.951289   14.660801\n17   198.160065   206.456879   223.376404   209.331116   12.851532\n18   192.999023   199.585693   214.821564   202.468760   11.193296\n19   188.964066   193.678009   207.394913   196.678996    9.574888\n20   185.447983   188.685455   201.650848   191.928095    8.574337\n21   182.461060   183.931168   196.110855   187.501027    7.492473\n22   179.547821   180.367401   191.526154   183.813792    6.691660\n23   176.994110   177.068344   187.107605   180.390020    5.817718\n24   174.702515   174.644058   183.528061   177.624878    5.112390\n25   172.765198   171.401520   179.856155   174.674291    4.539129\n26   170.967178   169.524918   176.644455   172.378850    3.763850\n27   168.970490   166.761307   173.834793   169.855530    3.618843\n28   167.218201   165.169189   171.618591   168.001994    3.295367\n29   165.483673   162.967484   168.668228   165.706462    2.856895\n30   164.443878   161.301773   166.160553   163.968735    2.463992\n31   162.735077   159.475647   163.974182   162.061635    2.323650\n32   161.037888   158.230453   161.182220   160.150187    1.664104\n33   159.931137   156.812210   158.970718   158.571355    1.597356\n34   158.441544   155.248871   156.696228   156.795547    1.598652\n35   157.061874   154.220444   154.595306   155.292542    1.543708\n36   155.781403   152.676346   152.288910   153.582219    1.914375\n37   154.578934   152.219925   149.775589   152.191483    2.401799\n38   153.178741   150.477234   147.747345   150.467773    2.715711\n39   152.234009   149.718765   145.235687   149.062820    3.544972\n40   150.944183   149.058334   142.997604   147.666707    4.152048\n41   150.069992   148.132904   140.731216   146.311371    4.928658\n42   149.187103   147.222626   138.725540   145.045090    5.560335\n43   147.846161   146.297012   136.333755   143.492310    6.247691\n44   147.202209   145.509750   134.421036   142.377665    6.942411\n45   146.175735   144.740646   132.423309   141.113230    7.559822\n46   145.314835   144.173340   129.806870   139.765015    8.642872\n47   144.797913   143.658554   127.691917   138.716128    9.564228\n48   143.775406   142.803192   125.174484   137.251027   10.469884\n49   142.894455   142.401550   123.065132   136.120379   11.308861"
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### How does the mean of the mean squared errors compare to that from Step B?"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The D step obtained the best results in comparison to the other steps, however a variation in the standard deviation is observed with behavior in its increasing and decreasing epochs."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}