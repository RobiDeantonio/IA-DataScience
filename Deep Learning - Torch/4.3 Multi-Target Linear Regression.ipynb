{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />\n\n\n\n<h1>Linear Regression with Multiple Outputs</h1> "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "\n# Table of Contents\nIn this lab, we will  review how to make a prediction for Linear Regression with Multiple Output. \n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n<li><a href=\"#ref2\">Build Custom Modules </a></li>\n\n<br>\n<p></p>\nEstimated Time Needed: <strong>15 min</strong>\n</div>\n\n<hr>"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"ref1\"></a>\n<h2 align=center>Class Linear  </h2>\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from torch import nn\nimport torch",
            "execution_count": 1,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Set the random seed:"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "torch.manual_seed(1)",
            "execution_count": 2,
            "outputs": [
                {
                    "data": {
                        "text/plain": "<torch._C.Generator at 0x7f52e4181170>"
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Set the random seed:"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "class linear_regression(nn.Module):\n    def __init__(self,input_size,output_size):\n        super(linear_regression,self).__init__()\n        self.linear=nn.Linear(input_size,output_size)\n    def forward(self,x):\n        yhat=self.linear(x)\n        return yhat",
            "execution_count": 3,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "create a linear regression  object, as our input and output will be two we set the parameters accordingly "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model=linear_regression(1,10)\nmodel(torch.tensor([1.0]))",
            "execution_count": 4,
            "outputs": [
                {
                    "data": {
                        "text/plain": "tensor([ 0.7926, -0.3920,  0.1714,  0.0797, -1.0143,  0.5097, -0.0608,  0.5047,\n         1.0132,  0.1887], grad_fn=<AddBackward0>)"
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "we can use the diagram to represent the model or object "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<img src = \"https://public.boxcloud.com/d/1/b1!v-FKNfHT1GX-o-21h8SCJnblw2_jvPKYNBjkwJqZbDWKEVfO26WhChG9cmiqzpkB-ROnoRw7VztzTh7raUUT-wZf_y1D9CL9D2fqefSy3Fedm7MpcFu-ymHDxiglal-tCAVxac6wWBCB-7Mce0T3ol3AIzz71eYoXIU_LObWpU-boF_Yd2L2bVskwB3NTzoNT1w3uLGkLayXhy06TWB9KmquQkHSlVCTwCqfgwu8_bX7CNzWuZnwXmCHP6oVuJCDNx6TZHTTYOkCoIK9NsXuJuBBCmME-fE2eZYxW2n0t3D-FW5Q8PHKTXj2VOpb70Vz_z0xUMjx7qzC4PuB_tmxiRdWaCNRrSzsfTRdUGLL7l-hkjOrjTIo4qTzEGIVKwimmbU1h9FsKcntGnOisF4O2D0POH5UbqMcZhAxkpHILZvkBeoa-4LuhAAmTFqeFL0DWSpv6fELbP7306uPpXwcXUWX4IMxxSdZjHNRUDSnC8W4aGLhmlGC0Uls3JZJv82upeUI6pBWPSNZxHieDmxFiiNXYF44Y9FXerVS8KjZbxYtAhnhW3AObmfav_3lvGzYx5jtEpdgzBdK_Q_BXRfB4oNTwgHfbQmviE-EClPqBbmwHeiEA2bEykTOAVW54wA-rUUNMInSPw3hQfnFoqd1eqxCOIgDGP_xt91eEJvzA7IKZkrh7r_0N54Z7Wt16qDkgPo05fZHKxiM6VlHN57sh_P0m-5GBV4WfTb81-nQXqmAJk-iDT8TmZt9cLFUUzZi8n6YeW6fy4fDZGzhwIYcvn10Y8gMBUwMh8ml-eZ2j0EZ1xkX55PeINtxzljMv0ADUubbMFlUZAqEMUgPKPpHptczMbjes5y0YhKe8xF8KdQN-MsciRIPWAKNT-lfMGV4xdbRu95leGbJbFl-8Ee-zicJz4idhelHDIQZI2Oppg7MO1ON-LSotpTCNnGVLetXke15h8DUoP4ou171d0QplEvTuawIzZRQ87-En3VJIsF1EfcBxfNpCM9Y7pFNPhQEvbIEyXR693fC84tZw616zQGJeC3xFIfJDdt9X73hZYHF317MvicJWfYW4rCi-frk6TAJqIZvp7ONEYmsecYyU-ISyARN5mBqGYMy0OKl1BxpKLV-BM0GbFTN4blzojbC8YDpmsqLUHQ_18mEMhoQkMAe4pzFCgQanQwKIvqDbSkdYa_W3FCvr8rSxmzjEOvb_sY25u4ideWB0G9_tNavmo2ihmnbANpm3UGeMtzC6vrc9MH-096v5-XS7Jevzolb5iA8ayFGO34I9IuzvrxAhgQnyvrdp1Bui4GY3LCf0aPB8VNzPnuyCG-J9ptvk5m9ZpDOebR1tPZ9B0ykchv22c6acVCQzz8Woq1P8gr2qApxbnq_9mS5OOUm9iT57dHS7GtnslLKO_TkgLRlDnDd5W4-vUma7ufAjo-Mqqs4gbOFYxkFJstiilBcB_Dz9BtN9dy9Q0Kvh9Ll/download\" width = 600, align = \"center\">"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "we can see the parameters "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "list(model.parameters())",
            "execution_count": 5,
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Parameter containing:\n tensor([[ 0.5153],\n         [-0.4414],\n         [-0.1939],\n         [ 0.4694],\n         [-0.9414],\n         [ 0.5997],\n         [-0.2057],\n         [ 0.5087],\n         [ 0.1390],\n         [-0.1224]], requires_grad=True),\n Parameter containing:\n tensor([ 0.2774,  0.0493,  0.3652, -0.3897, -0.0729, -0.0900,  0.1449, -0.0040,\n          0.8742,  0.3112], requires_grad=True)]"
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "we can create a tensor with two rows representing one sample of data"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "x=torch.tensor([[1.0]])",
            "execution_count": 6,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "we can make a prediction "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "yhat=model(x)\nyhat",
            "execution_count": 7,
            "outputs": [
                {
                    "data": {
                        "text/plain": "tensor([[ 0.7926, -0.3920,  0.1714,  0.0797, -1.0143,  0.5097, -0.0608,  0.5047,\n          1.0132,  0.1887]], grad_fn=<AddmmBackward>)"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "each row in the following tensor represents a different sample "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X=torch.tensor([[1.0],[1.0],[3.0]])",
            "execution_count": 8,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "we can make a prediction using multiple samples "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Yhat=model(X)\nYhat",
            "execution_count": 9,
            "outputs": [
                {
                    "data": {
                        "text/plain": "tensor([[ 0.7926, -0.3920,  0.1714,  0.0797, -1.0143,  0.5097, -0.0608,  0.5047,\n          1.0132,  0.1887],\n        [ 0.7926, -0.3920,  0.1714,  0.0797, -1.0143,  0.5097, -0.0608,  0.5047,\n          1.0132,  0.1887],\n        [ 1.8232, -1.2748, -0.2164,  1.0184, -2.8972,  1.7091, -0.4722,  1.5222,\n          1.2912, -0.0561]], grad_fn=<AddmmBackward>)"
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "the following figure represents the operation, where the red and blue  represents the different parameters, and the different shades of green represent  different samples."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": " <img src = \"https://public.boxcloud.com/d/1/b1!39S1xkcbOxyVs889Gni36MYbhv5LPutyWw7D46n9WwjcQkHm_0MLxCKSxkq61I8QpUI5cVi6QRxztgzwzxBBbAZuF65GNjaTIw8elki8bs7oi6Sfhvk1yeXNVADRRd7LOIR-aFumQhGqXNKdh5UDuegY_08O_0Fn1HyS1FiX5-pvLtu4qVwMB-hz51OQJJLbDOt93ZAX4w9YC5gxwqdzMviEXHpscCojtE9F5r_DOBvBF3NxyIBGv82lIzVm3c7tgJfoFQ0w41c6lkJmDM-9Rtc_Bq-Ne5hW4aSJzQIRvj2OZgvftTFOxg1SuwpOn9YOv2AeL4JIrPebY9IC4SsP1Xcfmt4kfMTmbdBk7Nn_JtJk0tBZpZjV0z6PG8UusyZY_rSrxxJ4JRVELmjBlfIm4Q_OriAiN_zXw2kjfshvqPwl6Jddv_wRdIUXqa9KMb-eKelazY4TMIlyQw3S0GD2z-c-0bbbjLcnWRv2yrZsq1xJhBGxAkLBcqI0kE5i-K9mLaI0XDPJs8nfBofN8i612qijbuPnEy4eyQl1LJ-p6VoVUGE_CWLaj-81ClTSu0NJxf0ULl8xdwqvA6wLgyKqskXTE8kpYZeDcU6myVJ6XV1b8Na-xNLQyIIs9PxF3RdrRsGJfWP-oXk-UmkrMQ4c4Zxgj_5Auxf7_7QQ6W9CivpTNMYfPleeoi635vdG_q4MKFsDJQZw7C5GYBTEPVU1iTcWNytMf8ESNhQ-JVyUuzCB5NZXRbf7TsoFGNbcoyGReIDPiDDthgoXd-aV62imppy47yaQH1Gf2f5ZRgfMESbIEOw_zzmkBUpQChx3zd1hSQsySQMJlIMgJKpNwsuIdXzNYOJ87HLxAYqKJI16r2ev6BCS87-PRMu8DgA7dHRkWNeMGX4FQ1f83pLwA6fshaKcFKb3wEte61eeI4bOJm3-_Up09i95FpUL0fzfh5PBHQeOa1PZF31ZKtGqGvut6a0lt5euED4DdrjNG4MLCFgaTcsAlvoHillrTmzqUu1vwR0a_Y6bjnvin_3AwCf3bzuAT7JqBjsshCW0RemWu9IPQI9GFMqYEKIeEo_WCo0Lxd93JOcJhyfpxxTR0HN6qrda2TCGbkzwsW7QcdE-ErvWM6aIi7MliUuPvTyCTjdUHAgWOkdv-VOvPWLSugx2tivd5NVVfBMMqw5oP9SZbZXtGiwV_p1PxyHUrE-Rzyz_pXuVi4JbqlXvQqboJ25FQ0bsLRIkCdJ8cw87Ie4ECuInusePck4rBi_xmgj-1L4cHg0mwSd_4dxPzyzYMdM-X60-NalAbNtyPqTNto6z5My9WlC0zFjKXaefPKYppJnvlA5OIgnKOCT7J5U7nsXeqz-SlLWBaNHfYD0ouvicEhHl8Kr2M90mzCTZYR5RhXy5OogjI2u8xhG3FV1E-lkWeBsYIJ7qP22CkeYLX7czev6Wre93_Ed3r0Nps-TR4djULJxypRPFJo1E/download\" width = 600, align = \"center\">"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a href=\"http://cocl.us/pytorch_link_top\">\n    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n</a> "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# About the Authors:  \n\n [Joseph Santarcangelo]( https://www.linkedin.com/in/joseph-s-50398b136/) has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n \nOther contributors: [Michelle Carey](  https://www.linkedin.com/in/michelleccarey/) "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}