{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import micasense.metadata as metadata\n",
    "import os,glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1.4373488426 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: %.10f seconds.\" % elapsed_time)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data :  {\"altitude\":2552.207,\"fix3d\":true,\"latitude\":0.08195555309493312,\"longitude\":-1.293311265327675,\"p_acc\":0,\"utc_time\":\"2020-10-30T15:10:36.999568Z\",\"v_acc\":5.657999992370605,\"vel_d\":0.17,\"vel_e\":0,\"vel_n\":0.05}\n",
      "\n",
      "Has 3d fix:  True\n",
      "{'id': 'DQrOWlIgqlfgFbp1TdNm', 'jpeg_cache_path': {'1': '/images/tmp26.jpg', '2': '/images/tmp25.jpg', '3': '/images/tmp27.jpg', '4': '/images/tmp28.jpg', '5': '/images/tmp29.jpg'}, 'jpeg_storage_path': {'1': '/files/0016SET/000/IMG_0002_1.jpg', '2': '/files/0016SET/000/IMG_0002_2.jpg', '3': '/files/0016SET/000/IMG_0002_3.jpg', '4': '/files/0016SET/000/IMG_0002_4.jpg', '5': '/files/0016SET/000/IMG_0002_5.jpg'}, 'raw_cache_path': {'1': '/images/tmp20.tif', '2': '/images/tmp21.tif', '3': '/images/tmp22.tif', '4': '/images/tmp23.tif', '5': '/images/tmp24.tif'}, 'raw_storage_path': {'1': '/files/0016SET/000/IMG_0002_1.tif', '2': '/files/0016SET/000/IMG_0002_2.tif', '3': '/files/0016SET/000/IMG_0002_3.tif', '4': '/files/0016SET/000/IMG_0002_4.tif', '5': '/files/0016SET/000/IMG_0002_5.tif'}, 'status': 'complete', 'time': '2020-10-30T15:10:36.999568Z'}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "##################################################################################\n",
    "# The MIT License (MIT)\n",
    "# \n",
    "# Copyright (c) 2015 MicaSense, Inc.\n",
    "# \n",
    "##################################################################################\n",
    "\n",
    "#Read the GPS data from the camera\n",
    "gps_data = requests.get('http://192.168.10.254/gps')\n",
    "print(\"Raw data : \", str(gps_data.text))\n",
    "print(\"Has 3d fix: \",str(gps_data.json()['fix3d']))\n",
    "\n",
    "\n",
    "#Configure parameter to the camera\n",
    "capture_params = { 'enabled_bands_raw' : 31}\n",
    "capture_data = requests.post(\"http://192.168.10.254/config\", json=capture_params)\n",
    "\n",
    "\n",
    "#Post a message to the camera commanding a capture, block until complete\n",
    "capture_params = { 'store_capture' : True, 'block' : True }\n",
    "capture_data = requests.post(\"http://192.168.10.254/capture\", json=capture_params)\n",
    "print(capture_data.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "datStr = str(capture_data.json()['jpeg_cache_path']['5'])\n",
    "\n",
    "data = requests.get('http://192.168.10.254{}'.format(datStr), stream=True)\n",
    "## option 1\n",
    "with open('IMG_0002_5.jpg', 'wb') as out_file:\n",
    "    shutil.copyfileobj(data.raw, out_file)\n",
    "del data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Image from the example RedEdge imageSet (see the ImageSet notebook) without RigRelatives.\n",
    "imagePath = os.path.expanduser(os.path.join('.'))\n",
    "imageNames = glob.glob(os.path.join(imagePath,'IMG_0007_*.jpg'))\n",
    "#panelNames = glob.glob(os.path.join(imagePath,'000','IMG_0000_*.tif'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading reference image :  IMG_0002_1.jpg\n",
      "Reading image to align :  IMG_0002_2.jpg\n",
      "Aligning images ...\n",
      "Saving aligned image :  aligned2.jpg\n",
      "Estimated homography : \n",
      " [[ 1.24503448e+00 -8.11844737e-02  6.95446890e+00]\n",
      " [ 3.50126001e-01  1.25155962e+00 -9.77242866e+01]\n",
      " [ 4.00498420e-04  1.22223854e-04  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "MAX_FEATURES = 500\n",
    "GOOD_MATCH_PERCENT = 0.15\n",
    "\n",
    "\n",
    "def alignImages(im1, im2):\n",
    "\n",
    "  # Convert images to grayscale\n",
    "  im1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "  im2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "  \n",
    "  # Detect ORB features and compute descriptors.\n",
    "  orb = cv2.ORB_create(MAX_FEATURES)\n",
    "  keypoints1, descriptors1 = orb.detectAndCompute(im1Gray, None)\n",
    "  keypoints2, descriptors2 = orb.detectAndCompute(im2Gray, None)\n",
    "  \n",
    "  # Match features.\n",
    "  matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "  matches = matcher.match(descriptors1, descriptors2, None)\n",
    "  \n",
    "  # Sort matches by score\n",
    "  matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "\n",
    "  # Remove not so good matches\n",
    "  numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "  matches = matches[:numGoodMatches]\n",
    "\n",
    "  # Draw top matches\n",
    "  imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "  cv2.imwrite(\"matches.jpg\", imMatches)\n",
    "  \n",
    "  # Extract location of good matches\n",
    "  points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "  points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "  for i, match in enumerate(matches):\n",
    "    points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "    points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "  \n",
    "  # Find homography\n",
    "  h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    "\n",
    "  # Use homography\n",
    "  height, width, channels = im2.shape\n",
    "  im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    "  \n",
    "  return im1Reg, h\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  \n",
    "  # Read reference image\n",
    "    refFilename = \"IMG_0002_1.jpg\"\n",
    "    print(\"Reading reference image : \", refFilename)\n",
    "    imReference = cv2.imread(refFilename, cv2.IMREAD_COLOR)\n",
    "\n",
    "  # Read image to be aligned\n",
    "    imFilename = \"IMG_0002_2.jpg\"\n",
    "    print(\"Reading image to align : \", imFilename);  \n",
    "    im = cv2.imread(imFilename, cv2.IMREAD_COLOR)\n",
    "  \n",
    "    print(\"Aligning images ...\")\n",
    "  # Registered image will be resotred in imReg. \n",
    "  # The estimated homography will be stored in h. \n",
    "    imReg, h = alignImages(im, imReference)\n",
    "  \n",
    "  # Write aligned image to disk. \n",
    "    outFilename = \"aligned2.jpg\"\n",
    "    print(\"Saving aligned image : \", outFilename); \n",
    "    cv2.imwrite(outFilename, imReg)\n",
    "\n",
    "  # Print estimated homography\n",
    "    print(\"Estimated homography : \\n\",  h)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer Vision Class by Jorge Martinez\n"
     ]
    }
   ],
   "source": [
    "class ProyectividadOpenCV():\n",
    "    \"\"\"\n",
    "    Esta es una clase para solucionar problemas con homografias\n",
    "    \"\"\"\n",
    "    # Atributos de la clase\n",
    "    error_reproyeccion = 4\n",
    "    #--------------------------------------------------------------------------    \n",
    "    def __init__(self):\n",
    "        \"\"\"Inicializador del objeto miembro de la clase\"\"\"\n",
    "        \n",
    "    \n",
    "    #--------------------------------------------------------------------------    \n",
    "    def coser_imagenes(self, ruta_img_base, ruta_img_adicional, radio = 0.75, error_reproyeccion = 4.0, coincidencias = False):\n",
    "        \"\"\"Metodo que carga una imagen desde una ruta en disco duro\"\"\"\n",
    "        \n",
    "        imagen_adicional = ruta_img_adicional\n",
    "        imagen_base = ruta_img_base\n",
    "        # Se obtienen los puntos deinter√©s\n",
    "        \n",
    "        (kpsBase, featuresBase) = self.obtener_puntos_interes(imagen_base)\n",
    "        (kpsAdicional, featuresAdicional) = self.obtener_puntos_interes(imagen_adicional)\n",
    "        # Se buscan las coincidencias\n",
    "        \n",
    "        M = self.encontrar_coincidencias(imagen_base, imagen_adicional, kpsBase, kpsAdicional, featuresBase, featuresAdicional, radio)\n",
    "        \n",
    "        if M is None:\n",
    "            return None\n",
    "        \n",
    "        # Se halla la homgrafia\n",
    "        (H, status) = self.encontrar_H_RANSAC(M, kpsBase, kpsAdicional, error_reproyeccion)\n",
    "              \n",
    "        # Organizando la imagen resultante\n",
    "        \n",
    "        result = cv2.warpPerspective(imagen_base, H, (imagen_base.shape[1], imagen_base.shape[0]))\n",
    "        result[0:imagen_adicional.shape[0], 0:imagen_adicional.shape[1]] = imagen_adicional\n",
    "\n",
    "        # check to see if the keypoint matches should be visualized\n",
    "        if coincidencias:\n",
    "            vis = self.dibujar_coincidencias(imagen_base, imagen_adicional, kpsBase, kpsAdicional, M, status)\n",
    "\n",
    "            # return a tuple of the stitched image and the\n",
    "            # visualization\n",
    "            return (result, vis)\n",
    "\n",
    "        # return the stitched image\n",
    "        return result\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    def estabilizador_imagen(self, imagen_base, imagen_a_estabilizar, radio = 0.75, error_reproyeccion = 4.0, coincidencias = False):\n",
    "        \"\"\"Esta clase devuelve una secuencia de im√°genes tomadas de la c√°mara estabilizada con respecto a la primera imagen\"\"\"\n",
    "        \n",
    "        # Se obtienen los puntos deinter√©s\n",
    "        \n",
    "        (kpsBase, featuresBase) = self.obtener_puntos_interes(imagen_base)\n",
    "        (kpsAdicional, featuresAdicional) = self.obtener_puntos_interes(imagen_a_estabilizar)\n",
    "        # Se buscan las coincidencias        \n",
    "        \n",
    "        M = self.encontrar_coincidencias(imagen_base, imagen_a_estabilizar, kpsBase, kpsAdicional, featuresBase, featuresAdicional, radio)\n",
    "        \n",
    "        if M is None:\n",
    "            print(\"pocas coincidencias\")\n",
    "            return None\n",
    "        \n",
    "        if len(M) > 4:\n",
    "            # construct the two sets of points\n",
    "            \n",
    "#            M2 = cv2.getPerspectiveTransform(ptsA,ptsB)\n",
    "            (H, status) = self.encontrar_H_RANSAC_Estable(M, kpsBase, kpsAdicional, error_reproyeccion)\n",
    "            estabilizada = cv2.warpPerspective(imagen_base,H,(imagen_base.shape[1],imagen_base.shape[0]))\n",
    "            return estabilizada\n",
    "        print(\"sin coincidencias\")\n",
    "        return None\n",
    "        \n",
    "            \n",
    "     \n",
    "    #--------------------------------------------------------------------------\n",
    "    def obtener_puntos_interes(self, imagen):\n",
    "        \"\"\"Se obtienen los puntos de interes cn SIFT\"\"\"\n",
    "        \n",
    "        descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "        (kps, features) = descriptor.detectAndCompute(imagen, None)\n",
    "        \n",
    "        return kps, features\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    def encontrar_coincidencias(self, img1, img2, kpsA, kpsB, featuresA, featuresB, ratio):\n",
    "        \"\"\"Metodo para estimar la homografia\"\"\"\n",
    "        \n",
    "        matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "        rawMatches = matcher.knnMatch(featuresA, featuresB, 2)\n",
    "        matches = []\n",
    "#        \n",
    "#        # loop over the raw matches\n",
    "        for m in rawMatches:\n",
    "#            # ensure the distance is within a certain ratio of each\n",
    "#            # other (i.e. Lowe's ratio test)\n",
    "            if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
    "                matches.append((m[0].trainIdx, m[0].queryIdx))\n",
    "        \n",
    "#        print (matches)\n",
    "        return matches\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    def encontrar_H_RANSAC(self, matches, kpsA, kpsB, reprojThresh):\n",
    "        \"\"\"Metodo para aplicar una H a una imagen y obtener la proyectividad\"\"\"\n",
    "\n",
    "        if len(matches) > 4:\n",
    "            # construct the two sets of points\n",
    "            ptsA = np.float32([kpsA[i].pt for (_, i) in matches])\n",
    "            ptsB = np.float32([kpsB[i].pt for (i, _) in matches])\n",
    "\n",
    "            # compute the homography between the two sets of points\n",
    "            (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC, reprojThresh)\n",
    "\n",
    "            # return the matches along with the homograpy matrix\n",
    "            # and status of each matched point\n",
    "            return (H, status)\n",
    "\n",
    "        # otherwise, no homograpy could be computed\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    def encontrar_H_RANSAC_Estable(self, matches, kpsA, kpsB, reprojThresh):\n",
    "        \"\"\"Metodo para aplicar una H a una imagen y obtener la proyectividad\"\"\"\n",
    "        \n",
    "        if len(matches) > 4:\n",
    "            # construct the two sets of points\n",
    "            ptsA = np.float32([kpsA[i].pt for (_, i) in matches])\n",
    "            ptsB = np.float32([kpsB[i].pt for (i, _) in matches])\n",
    "    \n",
    "            # compute the homography between the two sets of points\n",
    "            (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC, reprojThresh)\n",
    "            \n",
    "            return (H, status)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def dibujar_coincidencias(self, imagen_base, imagen_adicional, kpsA, kpsB, matches, status):\n",
    "        \n",
    "        (hA, wA) = imagen_base.shape[:2]\n",
    "        (hB, wB) = imagen_adicional.shape[:2]\n",
    "        vis = np.zeros((max(hA, hB), wA + wB, 3), dtype=\"uint8\")\n",
    "        vis[0:hA, 0:wA] = imagen_base\n",
    "        vis[0:hB, wA:] = imagen_adicional\n",
    "\n",
    "        # loop over the matches\n",
    "        for ((trainIdx, queryIdx), s) in zip(matches, status):\n",
    "            # only process the match if the keypoint was successfully\n",
    "            # matched\n",
    "            if s == 1:\n",
    "                # draw the match\n",
    "                ptA = (int(kpsA[queryIdx].pt[0]), int(kpsA[queryIdx].pt[1]))\n",
    "                ptB = (int(kpsB[trainIdx].pt[0]) + wA, int(kpsB[trainIdx].pt[1]))\n",
    "                cv2.line(vis, ptA, ptB, (0, 255, 0), 1)\n",
    "\n",
    "        # return the visualization\n",
    "        return vis\n",
    "    \n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    def encontrar_H_marcas(self, las_xprima, las_x):\n",
    "        \"\"\"Metodo para estimar la homografia\"\"\"\n",
    "        #Se utiliza 0 y no RANSAC porque deseo que utilice todos los puntos que se tienen\n",
    "        H, estado = cv2.findHomography(las_x, las_xprima, 0,0.1)\n",
    "        return H, estado\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    def estabilizar_desde_marcas(self, imagen, marcas_click, marcas_cad_mm):\n",
    "        \"\"\"Esta clase retorna una imagen estabilizada con base en una imagen abstraida delas marcas del cad dadas en mm\"\"\"\n",
    "        \n",
    "        #Lo primero es tratar la imagen entrante\n",
    "        blur = cv2.blur(imagen, (3,3))\n",
    "        hsv = cv2.cvtColor(blur,cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        #Se aplica un filtro de color verde para separar las marcas del fondo\n",
    "        thresh_marcas = cv2.inRange(hsv,np.array((49,50,50)), np.array((107, 255, 255)))\n",
    "        marcas_H = list()\n",
    "        cad_H = list()\n",
    "        #Se hace una busqueda de las marcas visibles en un radio de 30 pixelesy se filtranpor area para sacar los pares que permitiran hallar la homografia\n",
    "        for i in range(0,len(marcas_click)):\n",
    "#            print(i)\n",
    "            x_men=marcas_click[i][0]-10\n",
    "            x_may=marcas_click[i][0]+10\n",
    "            y_men=marcas_click[i][1]-10\n",
    "            y_may=marcas_click[i][1]+10\n",
    "            area_marca = thresh_marcas[y_men:y_may, x_men:x_may]\n",
    "            image_marcas, contours_marcas,hierarchy_marcas = cv2.findContours(area_marca,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            max_area = 65\n",
    "            best_cnt = 1\n",
    "            for cnt in contours_marcas:\n",
    "                area = 1\n",
    "                area = cv2.contourArea(cnt)\n",
    "#                print(contours_marcas)\n",
    "#                print(area,m)\n",
    "                if area > max_area and area < 85:\n",
    "                    max_area = area\n",
    "                    best_cnt = cnt\n",
    "                    # finding centroids of best_cnt and draw a circle there\n",
    "                    cM = cv2.moments(best_cnt)\n",
    "                    cx,cy = int(cM['m10']/cM['m00']), int(cM['m01']/cM['m00'])\n",
    "                    cx = x_men+cx\n",
    "                    cy = y_men+cy\n",
    "                    marcas_H.append((cx,cy))\n",
    "                    cad_H.append((marcas_cad_mm[i][0]+100,marcas_cad_mm[i][1]))\n",
    "#                    print(marcas_H,cad_H)\n",
    "                    \n",
    "                    \n",
    "        las_x = np.array(cad_H)\n",
    "        las_xprima = np.array(marcas_H)\n",
    "        print(las_x,las_xprima)\n",
    "        if len(las_xprima) > 4:\n",
    "            \n",
    "            H, estado = self.encontrar_H_marcas(las_x, las_xprima)\n",
    "\n",
    "            estabilizada = cv2.warpPerspective(imagen,H,(1200,1200))\n",
    "            \n",
    "            return estabilizada\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    def estabilizar_desde_centroides_marcas(self, imagen, marcas_click, marcas_cad_mm):\n",
    "        \"\"\"Esta clase retorna una imagen estabilizada con base en una imagen abstraida delas marcas del cad dadas en mm\"\"\"\n",
    "        \n",
    "        las_x = np.array(marcas_cad_mm)\n",
    "        las_xprima = np.array(marcas_click)\n",
    "        print(las_x,las_xprima)\n",
    "        if len(las_xprima) > 4:\n",
    "            \n",
    "            H, estado = self.encontrar_H_marcas(las_x, las_xprima)\n",
    "\n",
    "            estabilizada = cv2.warpPerspective(imagen,H,(650,650))\n",
    "            \n",
    "            return estabilizada\n",
    "        \n",
    "        return None\n",
    "            \n",
    "            \n",
    "    #--------------------------------------------------------------------------\n",
    "    def inicializar_marcas(self, img_base):\n",
    "        \"\"\"Permite al usuario hacer click en el centro apriximado de cada marca y las guarda en orden\"\"\"\n",
    "        \n",
    "        global puntos_click\n",
    "        \n",
    "        #Copiar la imagen original para poder escribir sobre ella\n",
    "        #Sin modificarla\n",
    "        imagen_conmarcas =self.img_base.copy()\n",
    "        \n",
    "        #Mostrar la imagen\n",
    "        cv2.namedWindow(\"Imagen_base\")\n",
    "        cv2.setMouseCallback(\"Imagen_base\", click_and_count)\n",
    "        \n",
    "        while True:\n",
    "            # Mostrar a imagen\n",
    "            cv2.imshow(\"Imagen_base\", imagen_conmarcas)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            \n",
    "            # Menu principal\n",
    "            #Si se presiona r resetee la imagen\n",
    "            if key == ord(\"r\"):\n",
    "                puntos_click = list()\n",
    "                imagen_conmarcas = self.img_base.copy()\n",
    "                \n",
    "            # Si se presiona q salir\n",
    "            elif key == ord(\"q\"):\n",
    "                return puntos_click\n",
    "                break\n",
    "        \n",
    "            # Graficar los puntos que hayan en puntos_click\n",
    "            if puntos_click:\n",
    "                for pts_id, coords in enumerate(puntos_click):\n",
    "                    #Coordenadas\n",
    "                    x, y = coords[0], coords[1]\n",
    "                    # Dibujar un circulo\n",
    "                    cv2.circle(imagen_conmarcas, (x, y), 5, (0,0,255), 5, 2)\n",
    "                    # Seleccionar una fuente\n",
    "                    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                    cv2.putText(imagen_conmarcas, str(pts_id+1), (x, y), font, 10, (0,0,255),5)\n",
    " \n",
    "\n",
    "\n",
    "    def img_alignment_sequoia(self, img_RGB, img_GRE, img_base_NIR, img_RED, img_REG, width, height):\n",
    "        \"\"\"This class takes the five images given by Sequoia Camera and makes a photogrammetric\n",
    "        alignment. Returns four images (GRE, NIR, RED, REG) aligned with the RGB image\"\"\"\n",
    "\n",
    "        # Makes resize to all images\n",
    "\n",
    "        b_RGB = cv2.resize(img_RGB, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "        b_GRE = cv2.resize(img_GRE, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "        base_NIR = cv2.resize(img_base_NIR, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "        b_RED = cv2.resize(img_RED, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "        b_REG = cv2.resize(img_REG, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # Makes a stabilization with NIR image like base\n",
    "\n",
    "        stb_GRE = self.estabilizador_imagen(b_GRE, base_NIR)\n",
    "        stb_RGB = self.estabilizador_imagen(b_RGB, base_NIR)\n",
    "        stb_RED = self.estabilizador_imagen(b_RED, base_NIR)\n",
    "        stb_REG = self.estabilizador_imagen(b_REG, base_NIR)\n",
    "\n",
    "        return stb_RGB, stb_GRE, base_NIR, stb_RED, stb_REG   \n",
    "#===========================================================================        \n",
    "def main():\n",
    "    \"\"\"Computer Vision Class\"\"\"\n",
    "    print(\"Computer Vision Class by Jorge Martinez\")\n",
    "\n",
    "#===========================================================================\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer Vision Class by Jorge Martinez\n"
     ]
    }
   ],
   "source": [
    "class ProyectividadOpenCV():\n",
    "    \"\"\"\n",
    "    Esta es una clase para solucionar problemas con homografias\n",
    "    \"\"\"\n",
    "    # Atributos de la clase\n",
    "    error_reproyeccion = 4\n",
    "    #--------------------------------------------------------------------------    \n",
    "    def __init__(self):\n",
    "        \"\"\"Inicializador del objeto miembro de la clase\"\"\"\n",
    "        \n",
    "    \n",
    "    #-------------------------------------------------------------------------- \n",
    "    def img_alignment_sequoia(self, img_base_NIR, img_RED, width, height):\n",
    "        \"\"\"This class takes the five images given by Sequoia Camera and makes a photogrammetric\n",
    "        alignment. Returns four images (GRE, NIR, RED, REG) aligned with the RGB image\"\"\"\n",
    "\n",
    "        # Makes resize to all images\n",
    "\n",
    "        base_NIR = cv2.resize(img_base_NIR, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "        b_RED = cv2.resize(img_RED, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # Makes a stabilization with NIR image like base\n",
    "\n",
    "        stb_RED = self.estabilizador_imagen(b_RED, base_NIR)\n",
    "\n",
    "        return base_NIR, stb_RED\n",
    "#===========================================================================        \n",
    "    \n",
    "    def estabilizador_imagen(self, imagen_base, imagen_a_estabilizar, radio = 0.75, error_reproyeccion = 4.0, coincidencias = False):\n",
    "        \"\"\"Esta clase devuelve una secuencia de im√°genes tomadas de la c√°mara estabilizada con respecto a la primera imagen\"\"\"\n",
    "        \n",
    "        # Se obtienen los puntos deinter√©s\n",
    "        \n",
    "        (kpsBase, featuresBase) = self.obtener_puntos_interes(imagen_base)\n",
    "        (kpsAdicional, featuresAdicional) = self.obtener_puntos_interes(imagen_a_estabilizar)\n",
    "        # Se buscan las coincidencias        \n",
    "        \n",
    "        M = self.encontrar_coincidencias(imagen_base, imagen_a_estabilizar, kpsBase, kpsAdicional, featuresBase, featuresAdicional, radio)\n",
    "        \n",
    "        if M is None:\n",
    "            print(\"pocas coincidencias\")\n",
    "            return None\n",
    "        \n",
    "        if len(M) > 4:\n",
    "            # construct the two sets of points\n",
    "            \n",
    "#            M2 = cv2.getPerspectiveTransform(ptsA,ptsB)\n",
    "            (H, status) = self.encontrar_H_RANSAC_Estable(M, kpsBase, kpsAdicional, error_reproyeccion)\n",
    "            estabilizada = cv2.warpPerspective(imagen_base,H,(imagen_base.shape[1],imagen_base.shape[0]))\n",
    "            return estabilizada\n",
    "        print(\"sin coincidencias\")\n",
    "        return None\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    def obtener_puntos_interes(self, imagen):\n",
    "        \"\"\"Se obtienen los puntos de interes cn SIFT\"\"\"\n",
    "        \n",
    "        descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "        (kps, features) = descriptor.detectAndCompute(imagen, None)\n",
    "        \n",
    "        return kps, features\n",
    "    \n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    def encontrar_coincidencias(self, img1, img2, kpsA, kpsB, featuresA, featuresB, ratio):\n",
    "        \"\"\"Metodo para estimar la homografia\"\"\"\n",
    "        \n",
    "        matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "        rawMatches = matcher.knnMatch(featuresA, featuresB, 2)\n",
    "        matches = []\n",
    "#        \n",
    "#        # loop over the raw matches\n",
    "        for m in rawMatches:\n",
    "#            # ensure the distance is within a certain ratio of each\n",
    "#            # other (i.e. Lowe's ratio test)\n",
    "            if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
    "                matches.append((m[0].trainIdx, m[0].queryIdx))\n",
    "        \n",
    "#        print (matches)\n",
    "        return matches\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    def encontrar_H_RANSAC_Estable(self, matches, kpsA, kpsB, reprojThresh):\n",
    "        \"\"\"Metodo para aplicar una H a una imagen y obtener la proyectividad\"\"\"\n",
    "        \n",
    "        if len(matches) > 4:\n",
    "            # construct the two sets of points\n",
    "            ptsA = np.float32([kpsA[i].pt for (_, i) in matches])\n",
    "            ptsB = np.float32([kpsB[i].pt for (i, _) in matches])\n",
    "    \n",
    "            # compute the homography between the two sets of points\n",
    "            (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC, reprojThresh)\n",
    "            \n",
    "            return (H, status)\n",
    "\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def ndviCalculation(self,img_RED, img_NIR, width, height):\n",
    "        \n",
    "        stb_NIR, stb_RED = self.img_alignment_sequoia(img_NIR, img_RED, width, height)\n",
    "        \n",
    "        red = np.array(stb_RED, dtype=float)\n",
    "        nir = np.array(stb_NIR, dtype=float)\n",
    "        \n",
    "        check = np.logical_and(red > 1,nir > 1)\n",
    "        \n",
    "        ndvi = np.where(check, (nir-red)/(nir+red), 0)\n",
    "        ndvi_index = ndvi\n",
    "        \n",
    "        if ndvi.min() < 0:\n",
    "            ndvi = ndvi + ndvi.min()*-1\n",
    "        \n",
    "        ndvi = (ndvi*255)/ ndvi.max()\n",
    "        ndvi = ndvi.round()\n",
    "        \n",
    "        ndvi_image = np.array(ndvi, dtype=np.uint8)\n",
    "        \n",
    "        return ndvi_index, ndvi_image\n",
    "#===========================================================================        \n",
    "def main():\n",
    "    \"\"\"Computer Vision Class\"\"\"\n",
    "    print(\"Computer Vision Class by Jorge Martinez\")\n",
    "\n",
    "#===========================================================================\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os.path\n",
    "#import serial\n",
    "import math\n",
    "from scipy.interpolate import interp1d\n",
    "from time import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "@author: jolumartinez\n",
    "\"\"\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os.path\n",
    "#import serial\n",
    "import math\n",
    "from scipy.interpolate import interp1d\n",
    "from time import time\n",
    "import cv2\n",
    "\n",
    "images_alignment = ProyectividadOpenCV()\n",
    "\n",
    "#It defines image size\n",
    "width, height = 700, 500\n",
    "\n",
    "nombre_carpeta = \"sequoia_images/\"\n",
    "\n",
    "#Reading images\n",
    "img_RGB = cv2.imread(nombre_carpeta + \"img_RGB.JPG\",0)\n",
    "img_GRE = cv2.imread(nombre_carpeta + \"img_GRE.TIF\",0)\n",
    "img_NIR = cv2.imread(nombre_carpeta + \"img_NIR.TIF\",0)\n",
    "img_RED = cv2.imread(nombre_carpeta + \"img_RED.TIF\",0)\n",
    "img_REG = cv2.imread(nombre_carpeta + \"img_REG.TIF\",0)\n",
    "\n",
    "\n",
    "\n",
    "merged_fix_bad = cv2.merge((img_RED, img_NIR, img_REG))\n",
    "cv2.imshow('frame', merged_fix_bad)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "stb_NIR, stb_RED = images_alignment.img_alignment_sequoia(img_NIR, img_RED, width, height)\n",
    "\n",
    "merged_fix_stb = cv2.merge((stb_RED, stb_NIR, stb_RED))\n",
    "cv2.imshow('frame', merged_fix_stb)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "ndvi_index, ndvi_image = images_alignment.ndviCalculation(img_RED, img_NIR, width, height)\n",
    "im_color = cv2.applyColorMap(ndvi_image, cv2.COLORMAP_RAINBOW)\n",
    "cv2.imshow('frame', im_color)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((3,3), np.uint8) \n",
    "\n",
    "images_alignment = ProyectividadOpenCV()\n",
    "\n",
    "#It defines image size\n",
    "width, height = 700, 500\n",
    "\n",
    "nombre_carpeta = \"\"\n",
    "img_NIR = cv2.imread(\"IMG_0002_4.jpg\")\n",
    "img_RED = cv2.imread(\"IMG_0002_3.jpg\")\n",
    "\n",
    "img_NIR = cv2.cvtColor(img_NIR, cv2.COLOR_BGR2GRAY)\n",
    "img_NIR = cv2.erode(img_NIR, kernel, iterations=1)\n",
    "cv2.imshow('Gris NIR', img_NIR)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "img_RED = cv2.cvtColor(img_RED, cv2.COLOR_BGR2GRAY)\n",
    "img_RED = cv2.erode(img_RED, kernel, iterations=1)\n",
    "cv2.imshow('Gris RED', img_RED)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "ndvi_index, ndvi_image = images_alignment.ndviCalculation(img_RED, img_NIR, width, height)\n",
    "im_color = cv2.applyColorMap(ndvi_image, cv2.COLORMAP_RAINBOW)\n",
    "cv2.imshow('frame', im_color)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 1280)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"IMG_0007_4.jpg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cv2.imshow('gray', img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Taking a matrix of size 5 as the kernel \n",
    "kernel = np.ones((3,3), np.uint8) \n",
    "  \n",
    "# The first parameter is the original image, \n",
    "# kernel is the matrix with which image is  \n",
    "# convolved and third parameter is the number  \n",
    "# of iterations, which will determine how much  \n",
    "# you want to erode/dilate a given image.  \n",
    "img_erosion = cv2.erode(img, kernel, iterations=1) \n",
    "  \n",
    "cv2.imshow('Input', img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.imshow('Erosion', img_erosion) \n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "#cap = cv2.VideoCapture(1)\n",
    "cap = cv2.imread(\"IMG_0002_4.jpg\")\n",
    "\n",
    "\n",
    "kernel11 = np.ones((3,),np.uint8)\n",
    "kernel9 = np.ones((9,9),np.uint8)\n",
    "ee3  = np.ones((3,3),np.uint8)\n",
    "ee5 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(41,41))\n",
    "\n",
    "while(True):\n",
    "    # Capturar frame por frame\n",
    "    #ret, frame = cap.read()\n",
    "    frame = cv2.resize(cap, (450,350))\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    apertura = cv2.morphologyEx(gray, cv2.MORPH_OPEN, kernel9)\n",
    "    cierre = cv2.morphologyEx(apertura, cv2.MORPH_CLOSE, kernel9)\n",
    "    erosion = cv2.erode(gray,kernel11,iterations = 1)\n",
    "    dilatacion = cv2.dilate(gray,kernel11,iterations = 1)\n",
    "    gradiente = cv2.morphologyEx(gray, cv2.MORPH_GRADIENT,ee3)\n",
    "    tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT,ee5)\n",
    "\n",
    "    #ubicacion y tamanio de las ventanas\n",
    "    cv2.imshow('gradiente',gradiente)\n",
    "    cv2.imshow('cierre',cierre)\n",
    "    cv2.imshow('apertura',apertura)\n",
    "    cv2.imshow('erosion',erosion)\n",
    "    cv2.imshow('dilatacion',dilatacion)\n",
    "    cv2.imshow('tophat',tophat)\n",
    "\n",
    "    cv2.moveWindow('gradiente', 0, 0)\n",
    "    cv2.moveWindow('cierre', 500, 0)\n",
    "    cv2.moveWindow('erosion',950,0)\n",
    "    cv2.moveWindow('dilatacion',500,400)\n",
    "    cv2.moveWindow('apertura', 0,400)\n",
    "    cv2.moveWindow('tophat', 950,400)\n",
    "\n",
    "    #evento tecla q, para el bucle\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# destruye las ventanas y libera la cam\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
