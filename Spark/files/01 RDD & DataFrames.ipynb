{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48a74aa",
   "metadata": {},
   "source": [
    "# RDD & DATAFRAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caffb4bb",
   "metadata": {},
   "source": [
    "Todas las aplicaciones en Spark poseen un manejador central de programa *(Driver)* y varios ejecutores que se crean a lo largo del clúster, estas son las computadoras que realizarán las tareas en paralelo y finalmente devolverán los valores al driver, la aplicación central.\n",
    "\n",
    "Para fines de este curso, debido a que se usa un modelo *stand-alone*, solo se contará con un driver y un ejecutor, ambos alojados en la misma computadora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1118f7b",
   "metadata": {},
   "source": [
    "### RDD\n",
    "Para poder realizar estas tareas, Spark posee desde su versión 1.0 los **RDD (Resilient Distributed Dataset)**, los cuales son tolerantes a fallos y pueden ser distribuidos a lo largo de los nodos del clúster.\n",
    "\n",
    "Los RDD pueden ser creados al cargar datos de manera distribuida, como es desde un HDFS, Cassanda, Hbase o cualquier sistema de datos soportado por Hadoop, pero también por colecciones de datos de Scala o Python, además de poder ser leídos desde archivos en el sistema local.\n",
    "\n",
    "En visión general, un RDD puede ser visto como un set de datos los cuales soportan solo dos tipos de operaciones: **transformaciones y acciones**.\n",
    "\n",
    "Las transformaciones permiten crear un nuevo RDD a partir de uno previamente existente, mientras que las acciones retornan un valor al driver de la aplicación. El núcleo de operación del paradigma de Spark es la ejecución perezosa (Lazy), es decir que las transformaciones solo serán calculadas posterior a una llamada de acción.\n",
    "\n",
    "Además, los RDD poseen una familiaridad con el paradigma orientado a objetos, lo cual permite que podamos realizar operaciones de bajo nivel a modo. *Map, filter y reduce* son tres de las operaciones más comunes.\n",
    "\n",
    "Una de las grandes ventajas que ofrecen los RDD es la compilación segura; por su particularidad de ejecución perezosa, se calcula si se generará un error o no antes de ejecutarse, lo cual permite identificar problemas antes de lanzar la aplicación. El pero que podemos encontrar con los RDD es que no son correctamente tratados por el *Garbage collector* y cuando las lógicas de operación se hacen complejas, su uso puede resultar poco práctico, aquí entran los DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6185634",
   "metadata": {},
   "source": [
    "### DataFrames\n",
    "\n",
    "Esos componentes fueron agregados en la versión 1.3 de Spark y pueden ser invocados con el contexto espacial de Spark SQL. Como indica su nombre, es un módulo especialmente desarrollado para ser ejecutado con instrucciones parecidas al SQL estándar.\n",
    "\n",
    "De la misma forma, como los RDD, estos pueden ser creados a partir de archivos, tablas tipo Hive, bases de datos externas y RDD o DataFrames existentes.\n",
    "\n",
    "El primer detalle que salta cuando creamos un DataFrame es que poseen columnas nombradas, lo que a nivel conceptual es como trabajar con un DataFrame de Pandas. Con la excepción que a nivel interno Spark trabaja con Scala, lo cual le asigna a cada columna el tipo de dato Row, un tipo especial de objeto sin tipo definido.\n",
    "\n",
    "Pero no es todo, los DataFrames implementan un sistema llamado **Catalyst**, el cual es un motor de optimización de planes de ejecución, parecido al que usan las bases de datos, pero diseñado para la cantidad de datos propia de Spark, aunado a eso, se tiene implementado un optimizador de memoria y consumo de CPU llamado **Tungsten**, el cual determina cómo se convertirán los planes lógicos creados por Catalyst a un plan físico.\n",
    "\n",
    "Ahora que conoces más sobre RDD y DataFrames es momento de comenzar a utilizarlos en operaciones. Acompáñame a la siguiente clase, empezaremos por ejecutar transformaciones y acciones con los RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90c372",
   "metadata": {},
   "source": [
    "## Importamos SparkContext y SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef8e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c947ead",
   "metadata": {},
   "source": [
    "## Creamos nuestra primer sesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf57045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/15 22:47:23 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 172.18.0.2 instead (on interface eth0)\n",
      "21/12/15 22:47:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/15 22:47:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/12/15 22:47:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local').appName('myFirstSession').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fabc990",
   "metadata": {},
   "source": [
    "## Terminamos la sesión actual\n",
    "\n",
    "No podemos tener mas de una sesión a la vez en nuestro notebook, por lo cual con el método 'stop' terminaremos la applicación.\n",
    "\n",
    "De la misma forma, al terminar una applicación, debemos de indicar explicitamente que termine. De otra forma no liberará los recursos asignados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed89c442",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab04944",
   "metadata": {},
   "source": [
    "## Creamos una sesión heredando los atributos de un contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c322973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/15 22:47:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(master='local', appName='myFirstSession')\n",
    "spark2 = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fc914f",
   "metadata": {},
   "source": [
    "## Creamos una sesión múltiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd8c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession2 = spark2.newSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46ae012",
   "metadata": {},
   "source": [
    "## Revisamos que los tres objetos apuntan a la misma aplicación\n",
    "\n",
    "Aprovechando la salida que nos ofrece, conocemos SparkUI, el monitor por excelencia para Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ed1e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>myFirstSession</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=myFirstSession>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ef3cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>myFirstSession</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f06c0737eb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00473852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>myFirstSession</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f06a80b0910>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8e849b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa4c4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614725a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
