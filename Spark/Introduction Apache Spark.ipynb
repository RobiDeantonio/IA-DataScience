{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "This notebook is designed to run in a IBM Watson Studio default runtime (NOT the Watson Studio Apache Spark Runtime as the default runtime with 1 vCPU is free of charge). Therefore, we install Apache Spark in local mode for test purposes only. Please don't use it in production.\n\nIn case you are facing issues, please read the following two documents first:\n\nhttps://github.com/IBM/skillsnetwork/wiki/Environment-Setup\n\nhttps://github.com/IBM/skillsnetwork/wiki/FAQ\n\nThen, please feel free to ask:\n\nhttps://coursera.org/learn/machine-learning-big-data-apache-spark/discussions/all\n\nPlease make sure to follow the guidelines before asking a question:\n\nhttps://github.com/IBM/skillsnetwork/wiki/FAQ#im-feeling-lost-and-confused-please-help-me\n\n\nIf running outside Watson Studio, this should work as well. In case you are running in an Apache Spark context outside Watson Studio, please remove the Apache Spark setup in the first notebook cells."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install pyspark==2.4.5\n!pip install --upgrade pip",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n\n\nif not ('sc' in locals() or 'sc' in globals()):\n    print('It seems you are note running in a IBM Watson Studio Apache Spark Notebook. You might be running in a IBM Watson Studio Default Runtime or outside IBM Waston Studio. Therefore installing local Apache Spark environment for you. Please do not use in Production')\n    \n    from pip import main\n    main(['install', 'pyspark==2.4.5'])\n    \n    try:\n        from pyspark import SparkContext, SparkConf\n        from pyspark.sql import SparkSession\n    except ImportError as e:\n        print('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')\n\n    sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n    \n    spark = SparkSession \\\n        .builder \\\n        .getOrCreate()",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "It seems you are note running in a IBM Watson Studio Apache Spark Notebook. You might be running in a IBM Watson Studio Default Runtime or outside IBM Waston Studio. Therefore installing local Apache Spark environment for you. Please do not use in Production\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
                    "name": "stderr"
                },
                {
                    "output_type": "stream",
                    "text": "Requirement already satisfied: pyspark==2.4.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (2.4.5)\nRequirement already satisfied: py4j==0.10.7 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pyspark==2.4.5) (0.10.7)\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "Welcome to exercise two of \u201cApache Spark for Scalable Machine Learning on BigData\u201d. In this exercise you\u2019ll apply the basics of functional and parallel programming. \n\nAgain, please use the following two links for your reference:\nhttps://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD\nhttps://spark.apache.org/docs/latest/rdd-programming-guide.html\n\nLet\u2019s actually create a python function which decides whether a value is greater than 50 (True) or not (False)."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rdd = sc.parallelize(range(100))",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# please replace $$ with the correct characters\nrdd.count()",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 3,
                    "data": {
                        "text/plain": "100"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rdd.sum()",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 4,
                    "data": {
                        "text/plain": "4950"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def gt50(i):\n    if i > 50:\n        return True\n    else:\n        return False",
            "execution_count": 5,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print(gt50(4))\nprint(gt50(51))",
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "False\nTrue\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Let\u2019s simplify this function"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def gt50(i):\n    return i > 50",
            "execution_count": 7,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print(gt50(4))\nprint(gt50(51))",
            "execution_count": 8,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "False\nTrue\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Now let\u2019s use the lambda notation to define the function."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "gt50 = lambda i: i > 50",
            "execution_count": 9,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print(gt50(4))\nprint(gt50(51))",
            "execution_count": 10,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "False\nTrue\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#let's shuffle our list to make it a bit more interesting\nfrom random import shuffle\nl = list(range(100))\nshuffle(l)\nrdd = sc.parallelize(l)",
            "execution_count": 11,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Let\u2019s filter values from our list which are equals or less than 50 by applying our \u201cgt50\u201d function to the list using the \u201cfilter\u201d function. Note that by calling the \u201ccollect\u201d function, all elements are returned to the Apache Spark Driver. This is not a good idea for BigData, please use \u201c.sample(10,0.1).collect()\u201d or \u201ctake(n)\u201d instead."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "rdd.filter(gt50).collect()",
            "execution_count": 12,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 12,
                    "data": {
                        "text/plain": "[62,\n 57,\n 66,\n 92,\n 93,\n 72,\n 58,\n 51,\n 79,\n 70,\n 82,\n 80,\n 95,\n 71,\n 59,\n 60,\n 73,\n 94,\n 56,\n 83,\n 85,\n 74,\n 69,\n 77,\n 61,\n 89,\n 84,\n 86,\n 76,\n 63,\n 96,\n 54,\n 52,\n 98,\n 75,\n 91,\n 65,\n 53,\n 64,\n 81,\n 88,\n 90,\n 78,\n 55,\n 99,\n 97,\n 67,\n 87,\n 68]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "We can also use the lambda function directly."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "rdd.filter(lambda i: i > 50).collect()",
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 13,
                    "data": {
                        "text/plain": "[62,\n 57,\n 66,\n 92,\n 93,\n 72,\n 58,\n 51,\n 79,\n 70,\n 82,\n 80,\n 95,\n 71,\n 59,\n 60,\n 73,\n 94,\n 56,\n 83,\n 85,\n 74,\n 69,\n 77,\n 61,\n 89,\n 84,\n 86,\n 76,\n 63,\n 96,\n 54,\n 52,\n 98,\n 75,\n 91,\n 65,\n 53,\n 64,\n 81,\n 88,\n 90,\n 78,\n 55,\n 99,\n 97,\n 67,\n 87,\n 68]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Let\u2019s consider the same list of integers. Now we want to compute the sum for elements in that list which are greater than 50 but less than 75. Please implement the missing parts. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rdd.filter(lambda x: x > 50).filter(lambda x: x < 75).sum()",
            "execution_count": 14,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 14,
                    "data": {
                        "text/plain": "1500"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You should see \"1500\" as answer. Now we want to know the sum of all elements. Please again, have a look at the API documentation and complete the code below in order to get the sum."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.sql import Row\n\ndf = spark.createDataFrame([Row(id=1, value='value1'),Row(id=2, value='value2')])\n\n# let's have a look what's inside\ndf.show()\n\n# let's print the schema\ndf.printSchema()",
            "execution_count": 15,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+---+------+\n| id| value|\n+---+------+\n|  1|value1|\n|  2|value2|\n+---+------+\n\nroot\n |-- id: long (nullable = true)\n |-- value: string (nullable = true)\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# register dataframe as query table\ndf.createOrReplaceTempView('df_view')\n\n# execute SQL query\ndf_result = spark.sql('select value from df_view where id=2')\n\n#\u00a0examine contents of result\ndf_result.show()\n\n# get result as string\ndf_result.first().value",
            "execution_count": 16,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+------+\n| value|\n+------+\n|value2|\n+------+\n\n",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 16,
                    "data": {
                        "text/plain": "'value2'"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Although we\u2019ll learn more about DataFrames next week, please try to find a way to count the rows in this DataFrame by looking at the API documentation. No worries, we\u2019ll cover DataFrames in more detail next week.\n\nhttps://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df.count()",
            "execution_count": 17,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 17,
                    "data": {
                        "text/plain": "2"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Math for Statistics"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rdd = sc.parallelize([101]+list(range(100))) #+[102,103]",
            "execution_count": 18,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Mean"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "sum = rdd.sum()\nn = rdd.count()\nmean = sum/n\nprint (mean)",
            "execution_count": 19,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "50.00990099009901\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Median"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#rdd.sortBy(lambda x:x).collect\n#rdd.sortBy(lambda x:x).zipWithIndex().collect()\nsorterAndIndex = rdd.sortBy(lambda x:x).zipWithIndex().map(lambda value : (value[1],value[0]))\nn = sorterAndIndex.count()\n#print (sorterAndIndex.collect())\n#print (n)\nif (n % 2 == 1):\n    index = (n-1)/2\n    print (sorterAndIndex.lookup(index))\nelse:\n    index1 = (n/2)-1\n    index2 = n/2\n    value1 = sorterAndIndex.lookup(index1)[0]\n    value2 = sorterAndIndex.lookup(index2)[0]\n    print ((value1+value2)/2)",
            "execution_count": 20,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "[50]\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 2nd moment - Standard Deviation"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rdd2 = sc.parallelize([49]*100+[100])\n\nsum = rdd2.sum()\nn = rdd2.count()\nmean = sum/n\nprint (mean)",
            "execution_count": 21,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "49.504950495049506\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from math import sqrt\nsd = sqrt(rdd2.map(lambda x:pow(x-mean,2)).sum()/n)\nsd",
            "execution_count": 22,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 22,
                    "data": {
                        "text/plain": "5.049504950495049"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 3rd moment - Skewness"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rdd3 = sc.parallelize(list(range(100))+[-1000]*1000)\n\n##Mean\nsum = rdd3.sum()\nn = rdd3.count()\nmean = sum/n\nprint ('mean:',mean)\n\nsd = sqrt(rdd3.map(lambda x:pow(x-mean,2)).sum()/n)\nprint ('sd:',sd)",
            "execution_count": 23,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "mean: -904.5909090909091\nsd: 301.8355450920072\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "skewness = 1/n*rdd3.map(lambda x:pow(x-mean,3)/pow(sd,3)).sum()\nskewness",
            "execution_count": 24,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 24,
                    "data": {
                        "text/plain": "2.8503857144434037"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 4th moment - Kurtosis"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "kurtosis = 1/n*rdd3.map(lambda x:pow(x-mean,4)/pow(sd,4)).sum()\nkurtosis",
            "execution_count": 25,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 25,
                    "data": {
                        "text/plain": "9.134733566834043"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Covariance"
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "rddX = sc.parallelize(range(100))\n#rddY = sc.parallelize(range(100))\nrddY = sc.parallelize(reversed(range(100)))\n\nimport random\n#rddX = sc.parallelize(random.sample(range(100),100))\n#rddY = sc.parallelize(random.sample(range(100),100))\n\n#rddX = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n#rddY = sc.parallelize([7,6,5,4,5,6,7,8,9,10])\n\nmeanX = rddX.sum()/rddX.count()\nmeanY = rddY.sum()/rddY.count()\n\nrddXY = rddY.zip(rddX)\nrddXY.take(10)\ncovXY = rddXY.map(lambda x : (x[0]-meanX)*(x[1]-meanY)).sum()/rddXY.count()\ncovXY",
            "execution_count": 27,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 27,
                    "data": {
                        "text/plain": "-833.25"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rddX.getNumPartitions()",
            "execution_count": 28,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 28,
                    "data": {
                        "text/plain": "2"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rddY.getNumPartitions()",
            "execution_count": 29,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 29,
                    "data": {
                        "text/plain": "2"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Correlation"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "n = rddXY.count()\nsdX = sqrt(rddX.map(lambda x:pow(x-meanX,2)).sum()/n)\nsdY = sqrt(rddY.map(lambda x:pow(x-meanY,2)).sum()/n)\nprint(sdX)\nprint(sdY)\n\ncorrXY = covXY/(sdX*sdY)\ncorrXY",
            "execution_count": 30,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "28.86607004772212\n28.86607004772212\n",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 30,
                    "data": {
                        "text/plain": "-1.0"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Correlation Matrix"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from math import sqrt\nfrom pyspark.mllib.stat import Statistics\n\ncolumn1 = sc.parallelize(range(100))\ncolumn2 = sc.parallelize(range(100,200))\ncolumn3 = sc.parallelize(reversed(range(100)))\ncolumn4 = sc.parallelize(random.sample(range(100),100))\n\ndata = column1.zip(column2).zip(column3).zip(column4).map(lambda dat: (dat[0][0][0],dat[0][0][1],dat[0][1],dat[1]))\nprint(Statistics.corr(data))",
            "execution_count": 31,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "[[ 1.          1.         -1.         -0.04724872]\n [ 1.          1.         -1.         -0.04724872]\n [-1.         -1.          1.          0.04724872]\n [-0.04724872 -0.04724872  0.04724872  1.        ]]\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Statistics and transfomrations using DataFrames"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n    \n# create a dataframe out of it\ndf = spark.read.parquet('hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')",
            "execution_count": 32,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "--2020-07-10 15:59:43--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.112.3\nConnecting to github.com (github.com)|140.82.112.3|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n--2020-07-10 15:59:43--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n--2020-07-10 15:59:43--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: \u2018hmp.parquet\u2019\n\n100%[======================================>] 932,997     --.-K/s   in 0.03s   \n\n2020-07-10 15:59:43 (29.8 MB/s) - \u2018hmp.parquet\u2019 saved [932997/932997]\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df.show()\ndf.printSchema()",
            "execution_count": 33,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+\n|  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 20 rows\n\nroot\n |-- x: integer (nullable = true)\n |-- y: integer (nullable = true)\n |-- z: integer (nullable = true)\n |-- source: string (nullable = true)\n |-- class: string (nullable = true)\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "spark.sql('select class,count(*) from df group by class').show()",
            "execution_count": 34,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+--------------+--------+\n|         class|count(1)|\n+--------------+--------+\n| Use_telephone|   15225|\n| Standup_chair|   25417|\n|      Eat_meat|   31236|\n|     Getup_bed|   45801|\n|   Drink_glass|   42792|\n|    Pour_water|   41673|\n|     Comb_hair|   23504|\n|          Walk|   92254|\n|  Climb_stairs|   40258|\n| Sitdown_chair|   25036|\n|   Liedown_bed|   11446|\n|Descend_stairs|   15375|\n|   Brush_teeth|   29829|\n|      Eat_soup|    6683|\n+--------------+--------+\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df.groupBy('class').count().show()",
            "execution_count": 35,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+--------------+-----+\n|         class|count|\n+--------------+-----+\n| Use_telephone|15225|\n| Standup_chair|25417|\n|      Eat_meat|31236|\n|     Getup_bed|45801|\n|   Drink_glass|42792|\n|    Pour_water|41673|\n|     Comb_hair|23504|\n|          Walk|92254|\n|  Climb_stairs|40258|\n| Sitdown_chair|25036|\n|   Liedown_bed|11446|\n|Descend_stairs|15375|\n|   Brush_teeth|29829|\n|      Eat_soup| 6683|\n+--------------+-----+\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Let\u2019s create a bar plot from this data. We\u2019re using the pixidust library, which is Open Source, because of its simplicity. But any other library like matplotlib is fine as well. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#!pip install pixiedust\nimport pixiedust\nfrom pyspark.sql.functions import col\ncounts = df.groupBy('class').count().orderBy('count')\ndisplay(counts)",
            "execution_count": 36,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Pixiedust database opened successfully\nTable VERSION_TRACKER created successfully\nTable METRICS_TRACKER created successfully\n\nShare anonymous install statistics? (opt-out instructions)\n\nPixieDust will record metadata on its environment the next time the package is installed or updated. The data is anonymized and aggregated to help plan for future releases, and records only the following values:\n\n{\n   \"data_sent\": currentDate,\n   \"runtime\": \"python\",\n   \"application_version\": currentPixiedustVersion,\n   \"space_id\": nonIdentifyingUniqueId,\n   \"config\": {\n       \"repository_id\": \"https://github.com/ibm-watson-data-lab/pixiedust\",\n       \"target_runtimes\": [\"Data Science Experience\"],\n       \"event_id\": \"web\",\n       \"event_organizer\": \"dev-journeys\"\n   }\n}\nYou can opt out by calling pixiedust.optOut() in a new cell.\n",
                    "name": "stdout"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "\n        <div style=\"margin:10px\">\n            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n            </a>\n            <span>Pixiedust version 1.1.18</span>\n        </div>\n        "
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "text": "\u001b[31mPixiedust runtime updated. Please restart kernel\u001b[0m\nTable SPARK_PACKAGES created successfully\nTable USER_PREFERENCES created successfully\nTable service_connections created successfully\n",
                    "name": "stdout"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "DataFrame[class: string, count: bigint]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "This looks nice, but it would be nice if we can aggregate further to obtain some quantitative metrics on the imbalance like, min, max, mean and standard deviation. If we divide max by min we get a measure called minmax ration which tells us something about the relationship between the smallest and largest class. Again, let\u2019s first use SQL for those of you familiar with SQL. Don\u2019t be scared, we\u2019re used nested sub-selects, basically selecting from a result of a SQL query like it was a table. All within on SQL statement."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "spark.sql('''\n    select \n        *,\n        max/min as minmaxratio -- compute minmaxratio based on previously computed values\n        from (\n            select \n                min(ct) as min, -- compute minimum value of all classes\n                max(ct) as max, -- compute maximum value of all classes\n                mean(ct) as mean, -- compute mean between all classes\n                stddev(ct) as stddev -- compute standard deviation between all classes\n                from (\n                    select\n                        count(*) as ct -- count the number of rows per class and rename it to ct\n                        from df -- access the temporary query table called df backed by DataFrame df\n                        group by class -- aggrecate over class\n                )\n        )   \n''').show()",
            "execution_count": 37,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+----+-----+------------------+------------------+-----------------+\n| min|  max|              mean|            stddev|      minmaxratio|\n+----+-----+------------------+------------------+-----------------+\n|6683|92254|31894.928571428572|21284.893716741157|13.80427951518779|\n+----+-----+------------------+------------------+-----------------+\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The same query can be expressed using the DataFrame API. Again, don\u2019t be scared. It\u2019s just a sequential expression of transformation steps. You now an choose which syntax you like better."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.sql.functions import col, min, max, mean, stddev\n\ndf \\\n    .groupBy('class') \\\n    .count() \\\n    .select([ \n        min(col(\"count\")).alias('min'), \n        max(col(\"count\")).alias('max'), \n        mean(col(\"count\")).alias('mean'), \n        stddev(col(\"count\")).alias('stddev') \n    ]) \\\n    .select([\n        col('*'),\n        (col(\"max\") / col(\"min\")).alias('minmaxratio')\n    ]) \\\n    .show()",
            "execution_count": 38,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+----+-----+------------------+------------------+-----------------+\n| min|  max|              mean|            stddev|      minmaxratio|\n+----+-----+------------------+------------------+-----------------+\n|6683|92254|31894.928571428572|21284.893716741157|13.80427951518779|\n+----+-----+------------------+------------------+-----------------+\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Now it\u2019s time for you to work on the data set. First, please create a table of all classes with the respective counts, but this time, please order the table by the count number, ascending."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "spark.sql('select class,count(*) as count from df group by class order by count').show()",
            "execution_count": 39,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+--------------+-----+\n|         class|count|\n+--------------+-----+\n|      Eat_soup| 6683|\n|   Liedown_bed|11446|\n| Use_telephone|15225|\n|Descend_stairs|15375|\n|     Comb_hair|23504|\n| Sitdown_chair|25036|\n| Standup_chair|25417|\n|   Brush_teeth|29829|\n|      Eat_meat|31236|\n|  Climb_stairs|40258|\n|    Pour_water|41673|\n|   Drink_glass|42792|\n|     Getup_bed|45801|\n|          Walk|92254|\n+--------------+-----+\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Pixiedust is a very sophisticated library. It takes care of sorting as well. Please modify the bar chart so that it gets sorted by the number of elements per class, ascending. Hint: It\u2019s an option available in the UI once rendered using the display() function."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.sql.functions import min\n\n# create a lot of distinct classes from the dataset\nclasses = [row[0] for row in df.select('class').distinct().collect()]\n\n# compute the number of elements of the smallest class in order to limit the number of samples per calss\nmin = df.groupBy('class').count().select(min('count')).first()[0]\n\n# define the result dataframe variable\ndf_balanced = None\n\n#\u00a0iterate over distinct classes\nfor cls in classes:\n    \n    #\u00a0only select examples for the specific class within this iteration\n    # shuffle the order of the elements (by setting fraction to 1.0 sample works like shuffle)\n    # return only the first n samples\n    df_temp = df \\\n        .filter(\"class = '\"+cls+\"'\") \\\n        .sample(False, 1.0) \\\n        .limit(min)\n    \n    # on first iteration, assing df_temp to empty df_balanced\n    if df_balanced == None:    \n        df_balanced = df_temp\n    # afterwards, append vertically\n    else:\n        df_balanced=df_balanced.union(df_temp)",
            "execution_count": 40,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Please verify, by using the code cell below, if df_balanced has the same number of elements per class. You should get 6683 elements per class."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "df_balanced.createOrReplaceTempView('df_balanced')\nspark.sql('select class,count(*) as count from df_balanced group by class').show()",
            "execution_count": 41,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+--------------+-----+\n|         class|count|\n+--------------+-----+\n| Use_telephone| 6683|\n| Standup_chair| 6683|\n|      Eat_meat| 6683|\n|     Getup_bed| 6683|\n|   Drink_glass| 6683|\n|    Pour_water| 6683|\n|     Comb_hair| 6683|\n|          Walk| 6683|\n|  Climb_stairs| 6683|\n| Sitdown_chair| 6683|\n|   Liedown_bed| 6683|\n|Descend_stairs| 6683|\n|   Brush_teeth| 6683|\n|      Eat_soup| 6683|\n+--------------+-----+\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Parallelism in Apache Spark - Exam"
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "## 2\ndf1 = spark.createDataFrame([1,2,4,5,34,1,32,4,34,2,1,3], \"int\").toDF(\"data\")\ndf1.printSchema()\n#df1.show()\n\ndf1.createOrReplaceTempView('df1')\nmean = spark.sql('select mean(data) as mean from df1').collect()[0]['mean']\nmean",
            "execution_count": 42,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "root\n |-- data: integer (nullable = true)\n\n",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 42,
                    "data": {
                        "text/plain": "10.25"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## 3.\nrdd = sc.parallelize([1,2,4,5,34,1,32,4,34,2,1,3])\n\nsorterAndIndex = rdd.sortBy(lambda x:x).zipWithIndex().map(lambda value : (value[1],value[0]))\nn = sorterAndIndex.count()\n#print (sorterAndIndex.collect())\n#print (n)\nif (n % 2 == 1):\n    index = (n-1)/2\n    print (sorterAndIndex.lookup(index))\nelse:\n    index1 = (n/2)-1\n    index2 = n/2\n    value1 = sorterAndIndex.lookup(index1)[0]\n    value2 = sorterAndIndex.lookup(index2)[0]\n    print ((value1+value2)/2)\n\n    \n#df1.approxQuantile(\"data\", [0.5], 0.0001)",
            "execution_count": 43,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "3.5\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## 5\ndf1 = spark.createDataFrame([34,1,23,4,3,3,12,4,3,1], \"int\").toDF(\"data\")\n\nstd = spark.sql('select std(data) as std from df1').collect()[0]['std']\nstd",
            "execution_count": 44,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 44,
                    "data": {
                        "text/plain": "13.987819376482204"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## 7\nspark.sql(\"\"\"\nSELECT \n    ( 1/count(data) ) * SUM ( POWER(data-%s,4)/POWER(%s,4) ) as kurtosis from df1\n                    \"\"\" %(mean,std)).first().kurtosis",
            "execution_count": 45,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 45,
                    "data": {
                        "text/plain": "1.9546000280182132"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## 8\nspark.sql(\"\"\"\nSELECT \n    ( 1/count(data) ) * SUM ( POWER(data-%s,3)/POWER(%s,3) ) as skewness from df1\n                    \"\"\" %(mean,std)).first().skewness",
            "execution_count": 46,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 46,
                    "data": {
                        "text/plain": "0.9917330791185353"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## 9\n#dfa = spark.createDataFrame( (1,2,3,4,5,6,7,8,9,10),(7,6,5,4,5,6,7,8,9,10), ).toDF([\"dataA\",\"datab\"])\n\nrddX = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\nrddY = sc.parallelize([7,6,5,4,5,6,7,8,9,10])\n\nmeanX = rddX.sum()/rddX.count()\nmeanY = rddY.sum()/rddY.count()\n\nrddXY = rddY.zip(rddX)\nrddXY.take(10)\ncovXY = rddXY.map(lambda x : (x[0]-meanX)*(x[1]-meanY)).sum()/rddXY.count()\nprint('covariance:',covXY)\n\n\nn = rddXY.count()\nsdX = sqrt(rddX.map(lambda x:pow(x-meanX,2)).sum()/n)\nsdY = sqrt(rddY.map(lambda x:pow(x-meanY,2)).sum()/n)\n#print(sdX)\n#print(sdY)\n\ncorrXY = covXY/(sdX*sdY)\nprint('correlation:',corrXY)",
            "execution_count": 47,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "covariance: 2.21\ncorrelation: 0.42945017416576226\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}